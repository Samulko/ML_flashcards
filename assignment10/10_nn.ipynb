{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Assignment 10\n",
        "\n",
        "Please add the name, first name, immatriculation number and study program below. Each member of the group has to be added:\n",
        "\n",
        "- Name: , First Name: , matr. number: , study program:.\n",
        "- Name:, First Name:, matr. number:, study program:.\n",
        "- Name:, First Name:, matr. number:, study program:."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from nn_helper import *\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Task 2: Single Neuron\n",
        "\n",
        "Before building complex networks, let's start with the fundamental building block: **a single neuron**. Understanding how one neuron works is crucial to understanding how thousands work together.\n",
        "\n",
        "**What is a Neuron?**\n",
        "- Takes multiple inputs, applies weights, adds bias, applies activation function\n",
        "- Formula: output = activation(w₁x₁ + w₂x₂ + ... + wₙxₙ + bias)\n",
        "- Can learn linear decision boundaries\n",
        "- Foundation of all neural networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (441428580.py, line 24)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexp_x =\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function: 1 / (1 + exp(-x))\n",
        "    \"\"\"\n",
        "    # Clip x to prevent overflow\n",
        "    x = np.clip(x, -500, 500)\n",
        "    # TODO: Implement the sigmoid function\n",
        "    return \n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of sigmoid function: sigmoid(x) * (1 - sigmoid(x))\n",
        "    \"\"\"\n",
        "    s = sigmoid(x)\n",
        "    # TODO: Implement the sigmoid derivative function\n",
        "    return\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Softmax activation for multi-class classification\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    # TODO: Implement the softmax function (subtract max for numerical stability)\n",
        "    exp_x = \n",
        "    return \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_input = np.array([-2, -1, 0, 1, 2])\n",
        "print(\"Sigmoid test:\", sigmoid(test_input))\n",
        "print(\"Softmax test:\", softmax(np.array([[1, 2, 3], [2, 1, 3]])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleNeuron:\n",
        "    \"\"\"\n",
        "    A single neuron - the fundamental building block of neural networks.\n",
        "    This neuron can learn to classify data into two classes (binary classification).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, learning_rate=0.1):\n",
        "        \"\"\"Initialize a single neuron.\"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # SOLUTION: Initialize weights and bias\n",
        "        self.weights = np.random.randn(input_size) * 0.1  # Small random weights\n",
        "        self.bias = 0.0  # Start with zero bias\n",
        "        \n",
        "        # For tracking training progress\n",
        "        self.loss_history = []\n",
        "        self.accuracy_history = []\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass: compute the neuron's output.\n",
        "        Formula: z = X @ weights + bias, then apply sigmoid activation\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        z = \n",
        "        \n",
        "        # TODO: Apply sigmoid activation\n",
        "        output = \n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Backward pass: update weights and bias using gradient descent.\n",
        "        \"\"\"\n",
        "        m = X.shape[0]  # number of samples\n",
        "        \n",
        "        # Compute gradients\n",
        "        # TODO: Compute the gradient dz\n",
        "        dz = \n",
        "        # TODO: Compute the weight gradients\n",
        "        dw = \n",
        "        # TODO: Compute the bias gradient\n",
        "        db = \n",
        "        \n",
        "        # TODO: Update the weights\n",
        "        self.weights -= \n",
        "        # TODO: Update the bias\n",
        "        self.bias -= \n",
        "    \n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        # TODO: Compute the binary cross-entropy loss\n",
        "        loss =\n",
        "        return loss\n",
        "    \n",
        "    def train(self, X, y, epochs=1000, verbose=True):\n",
        "        \"\"\"Train the single neuron.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "            \n",
        "            # Compute loss and accuracy\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            accuracy = np.mean((y_pred > 0.5).astype(int) == y)\n",
        "            \n",
        "            # Store metrics\n",
        "            self.loss_history.append(loss)\n",
        "            self.accuracy_history.append(accuracy)\n",
        "            \n",
        "            # Backward pass\n",
        "            self.backward(X, y, y_pred)\n",
        "            \n",
        "            # Print progress\n",
        "            if verbose and (epoch + 1) % 200 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions (0 or 1).\"\"\"\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and visualize the simple dataset\n",
        "X_simple, y_simple, feature_names, class_names = load_iris_dataset()\n",
        "visualize_iris_dataset(X_simple, y_simple, feature_names, class_names)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_simple_scaled = scaler.fit_transform(X_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a single neuron on this simple task\n",
        "single_neuron = SingleNeuron(input_size=2, learning_rate=0.1)\n",
        "single_neuron.train(X_simple_scaled, y_simple, epochs=500)\n",
        "\n",
        "# Test the neuron\n",
        "predictions = single_neuron.predict(X_simple_scaled)\n",
        "accuracy = np.mean(predictions == y_simple)\n",
        "print(f\"\\nFinal Accuracy: {accuracy:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "visualize_neuron_decision_boundary(single_neuron, X_simple, y_simple, scaler, feature_names, class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Now let's see what happens when we give our single neuron an impossible task: the famous XOR problem. This will demonstrate the fundamental limitation of single neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_xor, y_xor = create_xor_dataset()\n",
        "visualize_xor_problem(X_xor, y_xor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xor_neuron = SingleNeuron(input_size=2, learning_rate=1.0)\n",
        "xor_neuron.train(X_xor, y_xor, epochs=1000, verbose=True)\n",
        "\n",
        "xor_predictions = xor_neuron.predict(X_xor)\n",
        "xor_accuracy = np.mean(xor_predictions == y_xor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\\\nXOR Results with Single Neuron:\")\n",
        "print(\"Input | Target | Prediction | Correct?\")\n",
        "print(\"------|--------|------------|----------\")\n",
        "for i in range(len(X_xor)):\n",
        "    correct = \"✓\" if xor_predictions[i] == y_xor[i] else \"✗\"\n",
        "    print(f\"{X_xor[i]} |      {y_xor[i]} |          {xor_predictions[i]} | {correct}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {xor_accuracy:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Task 3: Multi-Layer Neural Networks\n",
        "\n",
        "The solution to XOR lies in **depth**! By adding hidden layers, we can create neural networks that learn non-linear patterns. Hidden layers can create internal representations that transform the problem into a linearly separable one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiLayerNeuralNetwork:\n",
        "    \"\"\"\n",
        "    A neural network with hidden layers that can solve non-linear problems.\n",
        "    This demonstrates the power of depth in neural networks.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
        "        \"\"\"Initialize a multi-layer neural network.\"\"\"\n",
        "        # First layer (input to hidden)\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        \n",
        "        # Second layer (hidden to output)\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_history = []\n",
        "        \n",
        "        print(f\"Created Multi-Layer Network:\")\n",
        "        print(f\"  Input Layer: {input_size} neurons\")\n",
        "        print(f\"  Hidden Layer: {hidden_size} neurons\")\n",
        "        print(f\"  Output Layer: {output_size} neuron(s)\")\n",
        "        print(f\"  Total Parameters: {(input_size * hidden_size + hidden_size) + (hidden_size * output_size + output_size)}\")\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        # TODO: Compute the linear combination for the hidden layer\n",
        "        self.z1 =\n",
        "        self.a1 =   # sigmoid activation\n",
        "        \n",
        "        # TODO: Compute the linear combination for the output layer\n",
        "        self.z2 = \n",
        "        if self.z2.shape[1] == 1:  # Binary classification\n",
        "            self.a2 =   # sigmoid\n",
        "        else:  # Multi-class classification\n",
        "            exp_z = np.exp(self.z2 - np.max(self.z2, axis=1, keepdims=True))\n",
        "            self.a2 =  # softmax\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def backward(self, X, y, output):\n",
        "        \"\"\"Backpropagation through multiple layers.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Ensure y is the right shape\n",
        "        if y.ndim == 1:\n",
        "            y = y.reshape(-1, 1)\n",
        "        \n",
        "        # Output layer gradients\n",
        "        # TODO: Compute the gradient dz2\n",
        "        dz2 = \n",
        "        # TODO: Compute the weight gradients dW2\n",
        "        dW2 = \n",
        "        # TODO: Compute the bias gradient db2\n",
        "        db2 = \n",
        "        \n",
        "        # Hidden layer gradients (this is where the magic happens!)\n",
        "        # TODO: Compute the gradient da1\n",
        "        da1 = \n",
        "        # TODO: Compute the gradient dz1\n",
        "        dz1 = \n",
        "        # TODO: Compute the weight gradients dW1\n",
        "        dW1 = \n",
        "        # TODO: Compute the bias gradient db1\n",
        "        db1 = \n",
        "        \n",
        "        # TODO: Update the weights and biases\n",
        "        self.W2 -= \n",
        "        self.b2 -= \n",
        "        self.W1 -= \n",
        "        self.b1 -= \n",
        "    \n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        if y_true.ndim == 1:\n",
        "            y_true = y_true.reshape(-1, 1)\n",
        "        # TODO: Compute the binary cross-entropy loss\n",
        "        return \n",
        "    \n",
        "    def train(self, X, y, epochs=2000, verbose=True):\n",
        "        \"\"\"Train the multi-layer network.\"\"\"\n",
        "        print(f\"\\nTraining for {epochs} epochs...\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = self.compute_loss(y, output)\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "            self.backward(X, y, output)\n",
        "            \n",
        "            if verbose and (epoch + 1) % 400 == 0:\n",
        "                accuracy = np.mean((output > 0.5).astype(int).flatten() == y.flatten())\n",
        "                print(f\"Epoch {epoch + 1:4d}: Loss = {loss:.6f}, Accuracy = {accuracy:.1%}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)\n",
        "    \n",
        "    def get_hidden_activations(self, X):\n",
        "        \"\"\"Get the hidden layer activations - useful for understanding what the network learned.\"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = 1 / (1 + np.exp(-np.clip(self.z1, -500, 500)))\n",
        "        return self.a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the multi-layer network\n",
        "xor_multi_nn = MultiLayerNeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=2.0)\n",
        "\n",
        "# Train it on XOR\n",
        "xor_multi_nn.train(X_xor, y_xor, epochs=2000, verbose=True)\n",
        "\n",
        "# Test the results\n",
        "multi_predictions = xor_multi_nn.predict(X_xor)\n",
        "multi_accuracy = np.mean(multi_predictions.flatten() == y_xor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"XOR RESULTS WITH MULTI-LAYER NETWORK:\")\n",
        "print(\"=\"*50)\n",
        "print(\"Input   | Target | Prediction | Success\")\n",
        "print(\"--------|--------|------------|--------\")\n",
        "for i in range(len(X_xor)):\n",
        "    success = \"✓\" if multi_predictions[i][0] == y_xor[i] else \"✗\"\n",
        "    print(f\"{X_xor[i]} |      {y_xor[i]} |          {multi_predictions[i][0]} | {success}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {multi_accuracy:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_xor_decision_boundary(xor_multi_nn, X_xor, y_xor)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "foml2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "assignment1_flashcards": [
    {
      "front": "When should you use median vs mean for missing value imputation?",
      "back": "Use median when data has outliers or is skewed; use mean for normally distributed data without outliers",
      "formula": "",
      "source": "Assignment 1 - Preprocessing",
      "tags": "preprocessing missing-values imputation",
      "extra": "PRACTICAL EXAMPLE: In Boston Housing dataset, median was chosen for RM, AGE, TAX because housing prices are skewed and contain outliers. Mean would be pulled by extreme values and give unrealistic imputations.\n\nDECISION FRAMEWORK:\n• Median: Robust to outliers, preserves typical values\n• Mean: Sensitive to outliers, assumes normal distribution\n• Mode: For categorical data\n\nIMPACT: Wrong choice can introduce bias and affect model performance downstream."
    },
    {
      "front": "What is the IQR method for outlier detection?",
      "back": "Data points outside Q1 - 1.5×IQR or Q3 + 1.5×IQR bounds are considered outliers",
      "formula": "\\[Outliers: x < Q1 - 1.5 \\times IQR \\text{ or } x > Q3 + 1.5 \\times IQR\\]",
      "source": "Assignment 1 - Preprocessing", 
      "tags": "preprocessing outliers IQR statistics",
      "extra": "ANALOGY: Like defining 'normal' height range - most people fall within expected range, very short or very tall people are outliers.\n\nPRACTICAL APPLICATION: In crime rate analysis (CRIM feature), IQR method identified neighborhoods with extremely high crime as outliers, but these may be valid data points representing dangerous areas.\n\nLIMITATIONS:\n• Assumes normal-ish distribution\n• May flag too many points in skewed data\n• Domain knowledge needed to decide if outliers are valid\n\nALTERNATIVES: Z-score method, modified Z-score, domain-specific thresholds"
    },
    {
      "front": "Why is data type correction important in preprocessing?",
      "back": "Incorrect data types (strings stored as objects) prevent mathematical operations and can cause silent failures in ML algorithms",
      "formula": "",
      "source": "Assignment 1 - Preprocessing",
      "tags": "preprocessing data-types pandas",
      "extra": "REAL-WORLD PROBLEM: In Boston Housing dataset, CRIM and ZN columns were stored as quoted strings ('0.02731') instead of floats.\n\nCONSEQUENCES OF NOT FIXING:\n• Mathematical operations fail silently\n• Algorithms may treat numbers as categories\n• Correlation analysis produces wrong results\n• Model training fails with cryptic errors\n\nSOLUTION PATTERN:\n1. Use df.info() to inspect data types\n2. Strip quotes with str.strip('\"')\n3. Convert with astype(float)\n4. Verify with df.info() again\n\nPREVENTION: Always check data types after loading any dataset"
    },
    {
      "front": "What does high correlation between features indicate and how should you handle it?", 
      "back": "High correlation indicates multicollinearity - features contain redundant information. Consider combining, removing one, or using regularization",
      "formula": "",
      "source": "Assignment 1 - Preprocessing",
      "tags": "preprocessing correlation multicollinearity feature-engineering",
      "extra": "EXAMPLE FROM ASSIGNMENT: TAX and RAD had 0.89 correlation in Boston Housing data - both relate to urban accessibility and development.\n\nWHY IT MATTERS:\n• Redundant features don't improve model performance\n• Can cause numerical instability in linear models\n• Makes feature importance interpretation difficult\n• Increases computational cost\n\nSTRATEGIES:\n• Remove one correlated feature\n• Combine features (e.g., accessibility_score = 0.5*TAX + 0.5*RAD)\n• Use regularization (Ridge/Lasso)\n• Apply PCA to reduce dimensions\n\nTHRESHOLD: Generally consider |correlation| > 0.8 as high"
    },
    {
      "front": "What is the main purpose of applying PCA before visualization?",
      "back": "PCA reduces high-dimensional data to 2D or 3D while preserving maximum variance, enabling visualization of patterns and clusters",
      "formula": "",
      "source": "Assignment 1 - PCA",
      "tags": "PCA dimensionality-reduction visualization",
      "extra": "PRACTICAL EXAMPLE: Iris dataset has 4 features (sepal/petal length/width) which can't be visualized directly. PCA transforms to 2D while keeping most information.\n\nKEY INSIGHT: Human brain can only visualize up to 3 dimensions effectively. PCA finds the 'best camera angles' to capture the most important patterns in high-dimensional data.\n\nWHAT'S PRESERVED:\n• Relative distances between points (approximately)\n• Cluster structures and separability\n• Major variance patterns\n\nWHAT'S LOST:\n• Exact feature interpretability\n• Some detailed variance (minor components)\n\nALTERNATIVES: t-SNE (non-linear), UMAP (preserves local structure)"
    },
    {
      "front": "How does K-means algorithm work using the EM framework?",
      "back": "E-step: Assign points to nearest centroids. M-step: Update centroids as mean of assigned points. Repeat until convergence",
      "formula": "\\[\\text{E-step: } c_i = \\arg\\min_k ||x_i - \\mu_k||^2\\]\\[\\text{M-step: } \\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i\\]",
      "source": "Assignment 1 - K-means",
      "tags": "k-means clustering EM-algorithm unsupervised-learning",
      "extra": "ANALOGY: Like organizing people at a party - people move to closest conversation group (E-step), then each group's center shifts to balance the conversation (M-step).\n\nPRACTICAL IMPLEMENTATION:\n1. Initialize k random centroids\n2. E-step: Calculate distances, assign points to nearest centroid\n3. M-step: Recalculate centroids as mean of assigned points\n4. Check convergence: if centroids barely moved, stop\n5. Otherwise repeat steps 2-4\n\nCONVERGENCE: Algorithm guaranteed to converge to local optimum, but result depends on initialization.\n\nEXAMPLE RESULT: Iris dataset converged in 9 iterations with 88.7% purity"
    },
    {
      "front": "What is clustering purity and how is it calculated?",
      "back": "Purity measures cluster homogeneity by calculating the percentage of points in each cluster that belong to the most common true class",
      "formula": "\\[\\text{Purity} = \\frac{1}{N} \\sum_{k=1}^{K} \\max_j |C_k \\cap T_j|\\]",
      "source": "Assignment 1 - Clustering Evaluation",
      "tags": "clustering evaluation purity unsupervised-learning",
      "extra": "INTUITIVE MEANING: 'How pure are my clusters?' - if a cluster contains mostly one type of flower, it has high purity.\n\nCOMPUTATION STEPS:\n1. For each cluster, count how many points belong to each true class\n2. Take the maximum count (dominant class) for each cluster\n3. Sum these maxima across all clusters\n4. Divide by total number of points\n\nEXAMPLE FROM ASSIGNMENT:\n• Cluster 0: 92.3% purity (mostly Setosa)\n• Cluster 1: 77.0% purity (mixed Versicolor/Virginica)\n• Cluster 2: 100% purity (pure Setosa)\n• Overall: 88.7% purity\n\nLIMITATIONS: Always ≤ 1.0, doesn't penalize having many clusters"
    },
    {
      "front": "Why did K-means struggle to separate Versicolor and Virginica iris species?",
      "back": "K-means assumes spherical clusters, but Versicolor and Virginica have overlapping, elongated distributions that violate this assumption",
      "formula": "",
      "source": "Assignment 1 - K-means Limitations",
      "tags": "k-means limitations clustering iris-dataset",
      "extra": "ALGORITHMIC LIMITATION: K-means uses Euclidean distance and assumes clusters are roughly circular/spherical with similar sizes.\n\nIRIS DATA CHARACTERISTICS:\n• Setosa: Well-separated, compact cluster (easy for K-means)\n• Versicolor & Virginica: Overlapping feature ranges, elongated distributions\n\nWHY K-MEANS FAILS:\n• Overlapping data: No clear separation boundary\n• Non-spherical shapes: Algorithm forces circular decision boundaries\n• Similar within-cluster variance assumption violated\n\nRESULT: Perfect clustering of Setosa (100% purity), mixed results for other species (77% purity)\n\nBETTER ALGORITHMS FOR THIS DATA:\n• Gaussian Mixture Models (handles elongated clusters)\n• Hierarchical clustering\n• DBSCAN (density-based)"
    }
  ]
}
{
  "assignment1_flashcards": [
    {
      "front": "When should you use median vs mean for missing value imputation?",
      "back": "Use median when data has outliers or is skewed; use mean for normally distributed data without outliers",
      "formula": "",
      "source": "Assignment 1 - Preprocessing",
      "tags": "preprocessing missing-values imputation",
      "extra": "ANALOGY: Like choosing between a typical employee salary (median) vs average salary (mean) - if the CEO makes $10M, the average is misleading but median shows what most people actually earn.\n\nPRACTICAL EXAMPLE: In Boston Housing dataset, median was chosen for RM, AGE, TAX because housing prices are skewed and contain outliers. Mean would be pulled by extreme values and give unrealistic imputations.\n\nKEY INSIGHT: Median is the 50th percentile - half the values are above, half below. Unaffected by extreme values.\n\nDECISION FRAMEWORK:\n• Median: Robust to outliers, preserves typical values, works with skewed distributions\n• Mean: Sensitive to outliers, assumes normal distribution, mathematically convenient\n• Mode: For categorical data or when most common value matters\n\nTECHNICAL: Missing values (NaN) are excluded from both calculations automatically in pandas.\n\nCONNECTIONS: Related to robust statistics, data quality assessment, feature engineering pipeline\n\nIMPACT: Wrong choice can introduce bias and affect model performance downstream - outlier-influenced means can create unrealistic imputations that models learn as 'normal'."
    },
    {
      "front": "What is the IQR method for outlier detection?",
      "back": "Data points outside Q1 - 1.5×IQR or Q3 + 1.5×IQR bounds are considered outliers",
      "formula": "\\[Outliers: x &lt; Q1 - 1.5 \\times IQR \\text{ or } x &gt; Q3 + 1.5 \\times IQR\\]",
      "source": "Assignment 1 - Preprocessing", 
      "tags": "preprocessing outliers IQR statistics",
      "extra": "ANALOGY: Like defining 'normal' height range - most people fall within expected range, very short or very tall people are outliers. The 1.5 multiplier creates a 'fence' around typical values.\n\nKEY INSIGHT: IQR (Interquartile Range) = Q3 - Q1, captures the middle 50% of data. The 1.5× multiplier is a conventional choice that works well empirically.\n\nPRACTICAL APPLICATION: In crime rate analysis (CRIM feature), IQR method identified neighborhoods with extremely high crime as outliers, but these may be valid data points representing dangerous areas.\n\nTECHNICAL PROCESS:\n1. Calculate Q1 (25th percentile) and Q3 (75th percentile)\n2. Compute IQR = Q3 - Q1\n3. Set fences: Lower = Q1 - 1.5×IQR, Upper = Q3 + 1.5×IQR\n4. Flag points outside fences as outliers\n\nLIMITATIONS:\n• Assumes normal-ish distribution\n• May flag too many points in skewed data\n• Domain knowledge needed to decide if outliers are valid\n• 1.5 multiplier is arbitrary (some use 2.0 or 3.0)\n\nCONNECTIONS: Related to boxplots (outliers shown as points), robust statistics, data quality assessment\n\nALTERNATIVES: Z-score method (uses standard deviations), modified Z-score (uses median absolute deviation), domain-specific thresholds"
    },
    {
      "front": "Why is data type correction important in preprocessing?",
      "back": "Incorrect data types (strings stored as objects) prevent mathematical operations and can cause silent failures in ML algorithms",
      "formula": "",
      "source": "Assignment 1 - Preprocessing",
      "tags": "preprocessing data-types pandas",
      "extra": "ANALOGY: Like trying to do math with words - '2' + '3' gives '23' (string concatenation) instead of 5 (addition). Computers need to know what type of data they're working with.\n\nREAL-WORLD PROBLEM: In Boston Housing dataset, CRIM and ZN columns were stored as quoted strings ('0.02731') instead of floats.\n\nKEY INSIGHT: Pandas reads CSV files and guesses data types. When numbers are quoted or contain special characters, they're interpreted as strings (object dtype).\n\nCONSEQUENCES OF NOT FIXING:\n• Mathematical operations fail silently or produce wrong results\n• Algorithms may treat numbers as categories (one-hot encoding nightmare)\n• Correlation analysis produces wrong results\n• Model training fails with cryptic errors\n• Statistical functions like mean(), std() don't work\n\nTECHNICAL SOLUTION PATTERN:\n1. Use df.info() to inspect data types\n2. Strip quotes with str.strip('\"') or str.replace('\"', '')\n3. Convert with astype(float) or pd.to_numeric()\n4. Verify with df.info() again\n\nCONNECTIONS: Related to data validation, ETL pipelines, data quality checks\n\nPREVENTION: Always check data types after loading any dataset - make it first step in EDA"
    },
    {
      "front": "What does high correlation between features indicate and how should you handle it?", 
      "back": "High correlation indicates multicollinearity - features contain redundant information. Consider combining, removing one, or using regularization",
      "formula": "",
      "source": "Assignment 1 - Preprocessing",
      "tags": "preprocessing correlation multicollinearity feature-engineering",
      "extra": "ANALOGY: Like having two thermometers measuring the same room temperature - they'll give nearly identical readings, so you only need one.\n\nEXAMPLE FROM ASSIGNMENT: TAX and RAD had 0.89 correlation in Boston Housing data - both relate to urban accessibility and development.\n\nKEY INSIGHT: Multicollinearity means features are linearly dependent - knowing one tells you about the other. Creates redundancy without adding information.\n\nWHY IT MATTERS:\n• Redundant features don't improve model performance\n• Can cause numerical instability in linear models (singular matrices)\n• Makes feature importance interpretation difficult\n• Increases computational cost and overfitting risk\n\nTECHNICAL STRATEGIES:\n• Remove one correlated feature (keep more interpretable one)\n• Combine features (e.g., accessibility_score = 0.5*TAX + 0.5*RAD)\n• Use regularization (Ridge/Lasso) to automatically handle correlation\n• Apply PCA to reduce dimensions while preserving information\n\nCONNECTIONS: Related to condition number, variance inflation factor (VIF), feature selection\n\nTHRESHOLD: Generally consider |correlation| > 0.8 as high, but domain-dependent"
    },
    {
      "front": "What is the main purpose of applying PCA before visualization?",
      "back": "PCA reduces high-dimensional data to 2D or 3D while preserving maximum variance, enabling visualization of patterns and clusters",
      "formula": "",
      "source": "Assignment 1 - PCA",
      "tags": "PCA dimensionality-reduction visualization",
      "extra": "ANALOGY: Like finding the best camera angles to photograph a 3D sculpture - you want views that capture the most detail with fewest shots. PCA finds the 'best angles' in data space.\n\nPRACTICAL EXAMPLE: Iris dataset has 4 features (sepal/petal length/width) which can't be visualized directly. PCA transforms to 2D while keeping most information.\n\nKEY INSIGHT: Human brain can only visualize up to 3 dimensions effectively. PCA projects high-dimensional data onto lower-dimensional subspace that captures maximum variance.\n\nTECHNICAL PROCESS:\n1. Standardize features (zero mean, unit variance)\n2. Compute covariance matrix\n3. Find eigenvectors (principal components)\n4. Project data onto top 2-3 components\n\nWHAT'S PRESERVED:\n• Relative distances between points (approximately)\n• Cluster structures and separability\n• Major variance patterns and relationships\n\nWHAT'S LOST:\n• Exact feature interpretability (components are linear combinations)\n• Some detailed variance (minor components discarded)\n\nCONNECTIONS: Related to SVD, eigendecomposition, manifold learning\n\nALTERNATIVES: t-SNE (non-linear, preserves local structure), UMAP (preserves both local and global structure)"
    },
    {
      "front": "How does K-means algorithm work using the EM framework?",
      "back": "E-step: Assign points to nearest centroids. M-step: Update centroids as mean of assigned points. Repeat until convergence",
      "formula": "\\[\\text{E-step: } c_i = \\arg\\min_k ||x_i - \\mu_k||^2\\]\\[\\text{M-step: } \\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i\\]",
      "source": "Assignment 1 - K-means",
      "tags": "k-means clustering EM-algorithm unsupervised-learning",
      "extra": "ANALOGY: Like organizing people at a party - people move to closest conversation group (E-step), then each group's center shifts to balance the conversation (M-step).\n\nPRACTICAL IMPLEMENTATION:\n1. Initialize k random centroids\n2. E-step: Calculate distances, assign points to nearest centroid\n3. M-step: Recalculate centroids as mean of assigned points\n4. Check convergence: if centroids barely moved, stop\n5. Otherwise repeat steps 2-4\n\nCONVERGENCE: Algorithm guaranteed to converge to local optimum, but result depends on initialization.\n\nEXAMPLE RESULT: Iris dataset converged in 9 iterations with 88.7% purity"
    },
    {
      "front": "What is clustering purity and how is it calculated?",
      "back": "Purity measures cluster homogeneity by calculating the percentage of points in each cluster that belong to the most common true class",
      "formula": "\\[\\text{Purity} = \\frac{1}{N} \\sum_{k=1}^{K} \\max_j |C_k \\cap T_j|\\]",
      "source": "Assignment 1 - Clustering Evaluation",
      "tags": "clustering evaluation purity unsupervised-learning",
      "extra": "ANALOGY: Like measuring how well you sorted colored balls into buckets - if each bucket contains mostly one color, you have high purity.\n\nINTUITIVE MEANING: 'How pure are my clusters?' - if a cluster contains mostly one type of flower, it has high purity.\n\nKEY INSIGHT: Purity is an extrinsic measure - requires ground truth labels. Measures how well clustering recovers true class structure.\n\nCOMPUTATION STEPS:\n1. For each cluster, count how many points belong to each true class\n2. Take the maximum count (dominant class) for each cluster\n3. Sum these maxima across all clusters\n4. Divide by total number of points\n\nTECHNICAL NOTATION:\n• C_k = points in cluster k\n• T_j = points in true class j\n• |C_k ∩ T_j| = points that are both in cluster k and true class j\n\nEXAMPLE FROM ASSIGNMENT:\n• Cluster 0: 92.3% purity (mostly Setosa)\n• Cluster 1: 77.0% purity (mixed Versicolor/Virginica)\n• Cluster 2: 100% purity (pure Setosa)\n• Overall: 88.7% purity\n\nLIMITATIONS: Always ≤ 1.0, doesn't penalize having many clusters (trivial solution: each point = one cluster gives purity = 1)\n\nCONNECTIONS: Related to accuracy, precision, Rand index, normalized mutual information"
    },
    {
      "front": "Why did K-means struggle to separate Versicolor and Virginica iris species?",
      "back": "K-means assumes spherical clusters, but Versicolor and Virginica have overlapping, elongated distributions that violate this assumption",
      "formula": "",
      "source": "Assignment 1 - K-means Limitations",
      "tags": "k-means limitations clustering iris-dataset",
      "extra": "ANALOGY: Like trying to separate two groups of people using only circular boundaries - works great if groups are naturally circular, but fails if groups are elongated or overlapping.\n\nALGORITHMIC LIMITATION: K-means uses Euclidean distance and assumes clusters are roughly circular/spherical with similar sizes and densities.\n\nKEY INSIGHT: K-means partitions space using Voronoi cells (regions closer to one centroid than others), which creates linear decision boundaries between centroids.\n\nIRIS DATA CHARACTERISTICS:\n• Setosa: Well-separated, compact cluster (easy for K-means)\n• Versicolor & Virginica: Overlapping feature ranges, elongated distributions\n\nWHY K-MEANS FAILS:\n• Overlapping data: No clear separation boundary exists\n• Non-spherical shapes: Algorithm forces circular decision boundaries\n• Similar within-cluster variance assumption violated\n• Equal cluster size assumption violated\n\nTECHNICAL: K-means minimizes within-cluster sum of squares, which naturally favors spherical clusters of similar size.\n\nRESULT: Perfect clustering of Setosa (100% purity), mixed results for other species (77% purity)\n\nCONNECTIONS: Related to curse of dimensionality, manifold learning, cluster validation\n\nBETTER ALGORITHMS FOR THIS DATA:\n• Gaussian Mixture Models (handles elongated clusters via covariance matrices)\n• Hierarchical clustering (can capture non-convex shapes)\n• DBSCAN (density-based, handles arbitrary shapes)"
    }
  ]
}
{
  "assignment5_flashcards": [
    {
      "front": "What is the fundamental independence assumption of Naïve Bayes classifiers?",
      "back": "Features are conditionally independent given the class label",
      "formula": "\\[P(X_1, X_2, ..., X_n | C) = \\prod_{i=1}^{n} P(X_i | C)\\]",
      "source": "Assignment 5 - Naïve Bayes Theory",
      "tags": "naive-bayes independence conditional-independence",
      "extra": "ANALOGY: Like medical symptoms - given you know the disease (class), fever doesn't tell you about cough likelihood. Each symptom is independent once diagnosis is known.\n\nKEY INSIGHT: This assumption is 'naïve' because features are often correlated in reality, but the classifier works surprisingly well despite this simplification.\n\nTECHNICAL MEANING:\n• P(fever, cough | flu) = P(fever | flu) × P(cough | flu)\n• Knowing fever status doesn't change cough probability, given flu diagnosis\n• Reduces parameter complexity from exponential to linear\n\nWHY IT WORKS DESPITE BEING WRONG:\n• Feature dependencies often cancel out across classes\n• Classifier only needs correct ranking, not precise probabilities  \n• Regularization effect prevents overfitting with limited data\n\nWHEN IT FAILS:\n• Strong feature correlations within classes\n• Features are deterministically related\n• Can lead to overconfident predictions\n\nCONNECTIONS: Related to Bayes' theorem, conditional independence, generative models\n\nPRACTICAL: Violations often don't hurt classification performance much, but affect probability calibration"
    },
    {
      "front": "How does parameter complexity scale for joint likelihood tables vs. Naïve Bayes?",
      "back": "Joint tables: exponential (2^n parameters), Naïve Bayes: linear (2n parameters) for binary features",
      "formula": "\\[\\text{Joint: } 2^n \\text{ vs. NB: } 2n \\text{ parameters}\\]",
      "source": "Assignment 5 - Complexity Analysis",
      "tags": "naive-bayes complexity scalability parameters",
      "extra": "ANALOGY: Like the difference between storing every possible combination in a phone book vs. just storing area codes and phone numbers separately - one explodes in size, the other stays manageable.\n\nKEY INSIGHT: The independence assumption trades modeling accuracy for computational tractability - this trade-off often favors Naïve Bayes in high-dimensional settings.\n\nTECHNICAL: For 10 binary features: joint table needs 1024 parameters, Naïve Bayes only 20. With limited training data, the parameter reduction often leads to better generalization despite the modeling bias.\n\nCONNECTIONS: Curse of dimensionality, bias-variance tradeoff, sample complexity theory\n\nPRACTICAL: This is why Naïve Bayes dominates in text classification and genomics - the independence assumption becomes a feature, not a bug, in high dimensions."
    },
    {
      "front": "What is Laplace smoothing and why is it essential for Naïve Bayes?",
      "back": "Add α (usually 1) to all counts to prevent zero probabilities for unseen feature-class combinations",
      "formula": "\\[P(X_i = v | C = c) = \\frac{\\text{count}(X_i = v, C = c) + \\alpha}{\\text{count}(C = c) + \\alpha |V_i|}\\]",
      "source": "Assignment 5 - Flu Detection",
      "tags": "laplace-smoothing naive-bayes zero-probability",
      "extra": "ANALOGY: Like adding a tiny amount of each ingredient to every recipe, even if you've never used it before - prevents complete recipe failure when you encounter new ingredients.\n\nKEY INSIGHT: Zero probabilities are catastrophic in Naïve Bayes because of multiplication - one zero kills the entire prediction.\n\nTECHNICAL: Without smoothing, if a feature value never appears with a class in training data, P(feature|class) = 0, making the entire prediction zero due to multiplication. This is catastrophic for new data.\n\nCONNECTIONS: Bayesian priors, regularization, pseudocounts, Dirichlet distributions\n\nPRACTICAL: α=1 is standard (add-one smoothing), larger α means more uniform distribution. Essential for robust performance on unseen data."
    },
    {
      "front": "How does Gaussian Naïve Bayes handle continuous features?",
      "back": "Assumes each feature follows a Gaussian distribution within each class, estimated by sample mean and variance",
      "formula": "\\[P(X_i = x | C = c) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ic}^2}} \\exp\\left(-\\frac{(x - \\mu_{ic})^2}{2\\sigma_{ic}^2}\\right)\\]",
      "source": "Assignment 5 - Gaussian Implementation",
      "tags": "gaussian-naive-bayes continuous-features probability-density",
      "extra": "ANALOGY: Like assuming all heights follow bell curves within each gender group - you estimate the average and spread for men vs. women separately.\n\nKEY INSIGHT: Gaussian assumption extends Naïve Bayes to continuous data by replacing counting with probability density estimation.\n\nTECHNICAL: For each class c and feature i, we estimate μ_ic (mean) and σ²_ic (variance) from training data. During prediction, we evaluate the Gaussian PDF at the test point.\n\nCONNECTIONS: Maximum likelihood estimation, normal distributions, parametric models\n\nPRACTICAL: Works for real-valued features like temperature, height, or medical measurements. The Gaussian assumption may not hold perfectly, but often works well. Standardization often improves performance."
    },
    {
      "front": "When does the conditional independence assumption of Naïve Bayes lead to high bias?",
      "back": "When features are strongly correlated within classes, creating dependency that violates the independence assumption",
      "formula": "\\[\\text{Bias occurs when: } X_1 \\not\\perp X_2 | C\\]",
      "source": "Assignment 5 - Independence Analysis",
      "tags": "naive-bayes bias independence-violation correlation",
      "extra": "ANALOGY: Like assuming exam scores in math and physics are independent for engineering students - clearly wrong since both depend on analytical skills.\n\nKEY INSIGHT: High bias occurs when the independence assumption is severely violated by strong within-class correlations.\n\nTECHNICAL: Example - blood pressure and cholesterol are positively correlated within age groups. Naïve Bayes will overweight evidence when both are high/low together.\n\nCONNECTIONS: Bias-variance tradeoff, model assumptions, feature engineering\n\nPRACTICAL: Consider feature selection or combining correlated features to reduce dependency violations."
    },
    {
      "front": "How do you compute posterior probabilities in Naïve Bayes classification?",
      "back": "Use Bayes' theorem with class priors and the product of feature likelihoods",
      "formula": "\\[P(C | X_1, ..., X_n) = \\frac{P(C) \\prod_{i=1}^n P(X_i | C)}{P(X_1, ..., X_n)}\\]",
      "source": "Assignment 5 - Bayes Classification",
      "tags": "bayes-theorem posterior-probability classification",
      "extra": "ANALOGY: Like a detective combining evidence - start with base probability (prior), then multiply by how likely each clue is given different suspects.\n\nKEY INSIGHT: Bayes' theorem turns generative modeling (how data is generated) into discriminative classification (which class is most likely).\n\nTECHNICAL: (1) Calculate priors P(C), (2) Calculate likelihoods P(X_i|C), (3) Multiply prior by all likelihoods, (4) Normalize or just compare unnormalized values.\n\nCONNECTIONS: Bayes' theorem, maximum a posteriori estimation, log-space computation\n\nPRACTICAL: Use log-probabilities to avoid numerical underflow with many features. Denominator often ignored for classification."
    },
    {
      "front": "What is the relationship between marginal and conditional independence?",
      "back": "Neither direction holds in general: X₁ ⊥ X₂ does not imply X₁ ⊥ X₂|C, and X₁ ⊥ X₂|C does not imply X₁ ⊥ X₂",
      "formula": "\\[X_1 \\perp X_2 \\not\\Rightarrow X_1 \\perp X_2 | C \\text{ and } X_1 \\perp X_2 | C \\not\\Rightarrow X_1 \\perp X_2\\]",
      "source": "Assignment 5 - Independence Theory",
      "tags": "independence conditional-independence marginal-independence",
      "extra": "ANALOGY: Like height and basketball skill - independent overall, but given NBA players, they're correlated (confounding by selection).\n\nKEY INSIGHT: Conditioning can create or destroy independence relationships - neither direction is guaranteed.\n\nTECHNICAL: Counterexample - fever and headache are conditionally independent given flu (both caused by flu), but marginally dependent (co-occur during flu season).\n\nCONNECTIONS: Causal inference, Simpson's paradox, graphical models\n\nPRACTICAL: This subtlety is crucial for understanding when Naïve Bayes assumptions are violated."
    },
    {
      "front": "How does missing data affect Naïve Bayes classification?",
      "back": "Simply omit the missing features from the likelihood calculation - the independence assumption makes this straightforward",
      "formula": "\\[P(C | X_{observed}) \\propto P(C) \\prod_{i \\in observed} P(X_i | C)\\]",
      "source": "Assignment 5 - Missing Data Handling",
      "tags": "missing-data naive-bayes robustness",
      "extra": "ANALOGY: Like a recipe where you can skip missing ingredients without affecting how the remaining ones combine - independence makes substitution easy.\n\nKEY INSIGHT: The independence assumption makes Naïve Bayes naturally robust to missing data - just omit the missing features from calculation.\n\nTECHNICAL: When thermometer fails, exclude temperature and use remaining features (fever, cough). Missing features don't affect relationships between observed features.\n\nCONNECTIONS: Missing data mechanisms, robustness, feature importance\n\nPRACTICAL: Major advantage over many classifiers. However, fewer features means each remaining one has more weight in the decision."
    }
  ]
}
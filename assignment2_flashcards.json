{
  "assignment2_flashcards": [
    {
      "front": "What is the relationship between PCA's first principal component and the covariance matrix?",
      "back": "The first principal component is the eigenvector of the covariance matrix corresponding to the largest eigenvalue",
      "formula": "\\[Cov(X) \\cdot v_1 = \\lambda_1 \\cdot v_1\\]",
      "source": "Assignment 2 - PCA Theory",
      "tags": "PCA eigenvector covariance-matrix dimensionality-reduction",
      "extra": "ANALOGY: Think of eigenvectors as the 'natural directions' of data spread, like finding the main axis of a stretched rubber band or the direction a football naturally tumbles.\n\nKEY INSIGHT: The covariance matrix encodes how features vary together. Its eigendecomposition reveals the fundamental directions of data variation.\n\nTECHNICAL PROCESS:\n1. Compute covariance matrix from mean-centered data\n2. Find eigenvalues and eigenvectors of covariance matrix\n3. Sort eigenvectors by eigenvalue (largest first)\n4. First principal component = eigenvector with largest eigenvalue\n\nMATHEMATICAL MEANING: Eigenvalue equation Av = λv means vector v doesn't change direction when transformed by matrix A, only gets scaled by λ.\n\nPRACTICAL: This relationship ensures PCA finds the optimal low-dimensional representation that preserves maximum variance in the data.\n\nCONNECTIONS: Related to SVD (Singular Value Decomposition), spectral analysis, matrix diagonalization"
    },
    {
      "front": "What do eigenvalues represent in PCA and how do you use them for component selection?",
      "back": "Eigenvalues represent the amount of variance explained by each principal component. Select components by calculating cumulative variance percentage until reaching desired threshold (e.g., 70%)",
      "formula": "\\[\\text{Variance Explained} = \\frac{\\lambda_i}{\\sum_{j=1}^{n} \\lambda_j} \\times 100\\%\\]",
      "source": "Assignment 2 - PCA Component Selection",
      "tags": "PCA eigenvalues variance-explained component-selection",
      "extra": "ANALOGY: Eigenvalues are like importance scores for each direction of data spread - bigger eigenvalues mean more important directions, like the major vs minor axes of an ellipse.\n\nKEY INSIGHT: Each eigenvalue measures the variance of data when projected onto its corresponding eigenvector. Total variance = sum of all eigenvalues.\n\nCOMPONENT SELECTION METHODS:\n• Cumulative Variance: Keep components until 70-95% variance explained\n• Kaiser Criterion: Keep components with eigenvalues > 1 (more than average)\n• Scree Plot: Look for 'elbow' where slope flattens dramatically\n• Cross-validation: Use validation performance to select optimal number\n\nTECHNICAL PROCESS:\n1. Rank eigenvalues from largest to smallest\n2. Calculate each component's variance percentage\n3. Compute cumulative percentage\n4. Choose cutoff (e.g., 80% cumulative variance)\n\nPRACTICAL TRADE-OFF: More components = more information retained but less dimensionality reduction achieved\n\nCONNECTIONS: Related to information theory, compression, noise reduction"
    },
    {
      "front": "How do you initialize cluster centers in K-means and why does initialization matter?",
      "back": "Common methods: random selection, k-means++, or domain knowledge. Initialization matters because K-means can converge to local optima, leading to poor clustering results",
      "formula": "",
      "source": "Assignment 2 - K-means Initialization",
      "tags": "k-means clustering initialization local-optima",
      "extra": "ANALOGY: Think of K-means initialization like choosing starting points for a treasure hunt - bad starting points can lead you to the wrong treasure, even if you follow the algorithm perfectly.\n\nKEY INSIGHT: K-means is a greedy algorithm that can get stuck in local optima. Good initialization increases chances of finding global optimum.\n\nINITIALIZATION METHODS:\n• Random: Simple but can lead to poor clustering\n• K-means++: Chooses centers far apart probabilistically - much better results\n• Domain knowledge: Use understanding of data structure\n• Multiple runs: Run algorithm several times, pick best result\n\nTECHNICAL DETAILS:\n• K-means++ chooses first center randomly, then each subsequent center with probability proportional to squared distance from nearest existing center\n• 'Best result' typically means lowest within-cluster sum of squares (WCSS)\n\nWHY IT MATTERS: Poor initialization can trap algorithm in local optima (finding a small hill instead of the mountain)\n\nCONNECTIONS: Related to optimization theory, random restarts, global vs local optimization\n\nPRACTICAL: Most implementations (scikit-learn) use K-means++ by default and run multiple times"
    },
    {
      "front": "What is the K-means algorithm's step-by-step process?",
      "back": "1) Initialize k cluster centers 2) Assign each point to nearest center 3) Update centers to cluster centroids 4) Repeat steps 2-3 until convergence (centers stop moving significantly)",
      "formula": "\\[\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\\]",
      "source": "Assignment 2 - K-means Algorithm",
      "tags": "k-means clustering algorithm centroid-update",
      "extra": "ANALOGY: K-means is like organizing people into groups at a party. Start with k party hosts (centers) in random locations. Each guest (data point) joins the nearest host. Then hosts move to the center of their groups (centroid). Repeat until hosts stop moving much.\n\nKEY INSIGHT: This is Lloyd's algorithm - alternates between assignment (E-step) and update (M-step), guaranteed to converge to local optimum.\n\nDETAILED ALGORITHM:\n1. Initialize k cluster centers μ₁, μ₂, ..., μₖ\n2. E-step (Assignment): For each point xᵢ, assign to cluster j = argmin ||xᵢ - μⱼ||²\n3. M-step (Update): For each cluster j, update μⱼ = mean of all points assigned to cluster j\n4. Check convergence: if centers move less than threshold, stop\n5. Otherwise, return to step 2\n\nTECHNICAL NOTES:\n• Uses Euclidean distance by default\n• Centroid update minimizes within-cluster sum of squares\n• Convergence guaranteed but only to local optimum\n\nOBJECTIVE: Minimizes within-cluster distances - everyone wants to be close to their group's center\n\nCONNECTIONS: Related to EM algorithm, Lloyd's algorithm, vector quantization\n\nCONVERGENCE: Happens when the 'social groups' stabilize and no one wants to switch teams anymore"
    },
    {
      "front": "What is the Dunn Index and what does a higher value indicate?",
      "back": "Dunn Index = minimum inter-cluster distance / maximum intra-cluster distance. Higher values indicate better clustering with well-separated, compact clusters",
      "formula": "\\[\\text{Dunn Index} = \\frac{\\min_{i \\neq j} d(C_i, C_j)}{\\max_k \\Delta(C_k)}\\]",
      "source": "Assignment 2 - Clustering Evaluation",
      "tags": "dunn-index clustering-evaluation inter-cluster intra-cluster",
      "extra": "ANALOGY: The Dunn Index is like measuring the quality of city neighborhoods - you want neighborhoods (clusters) to be tight-knit internally but well-separated from each other.\n\nKEY INSIGHT: Measures the ratio of cluster separation to cluster compactness. Higher values = better clustering quality.\n\nTECHNICAL COMPONENTS:\n• Numerator: minimum inter-cluster distance (how far apart closest clusters are)\n• Denominator: maximum intra-cluster distance (how spread out the most scattered cluster is)\n• Result: dimensionless ratio where higher = better\n\nINTERPRETA2TION:\n• High Dunn Index: Clear boundaries between groups, like distinct neighborhoods\n• Low Dunn Index: Clusters overlap or are internally scattered\n• Perfect clustering: compact clusters far apart = high Dunn Index\n\nCOMPUTATIONAL COMPLEXITY: O(n²) - must compute all pairwise distances\n\nCONNECTIONS: Related to silhouette coefficient, Calinski-Harabasz index, gap statistic\n\nPRACTICAL: Good for comparing different clustering algorithms on same dataset"
    },
    {
      "front": "What are the main limitations of the Dunn Index for clustering evaluation?",
      "back": "1) Sensitive to outliers and noise 2) Computationally expensive (O(n²)) 3) May not work well with non-spherical clusters 4) Single outlier can dramatically reduce the index",
      "formula": "",
      "source": "Assignment 2 - Clustering Evaluation Limitations",
      "tags": "dunn-index limitations clustering-evaluation outliers",
      "extra": "The Dunn Index is like judging a school by its worst student and best class - not always fair! A single outlier can make the minimum inter-cluster distance tiny, ruining the score even if most clusters are well-separated. It assumes clusters are round (spherical), so it struggles with chain-like or irregular shaped clusters. For large datasets, computing all pairwise distances becomes prohibitively slow. Alternative metrics like Silhouette coefficient or Calinski-Harabasz index often provide more robust evaluations."
    },
    {
      "front": "How does PCA projection reveal patterns in data, and what do projected values represent?",
      "back": "PCA projection transforms data into principal component space. Projected values represent data points' coordinates along the new axes, revealing the main patterns of variation",
      "formula": "\\[y = X \\cdot v\\]",
      "source": "Assignment 2 - PCA Interpretation",
      "tags": "PCA projection data-patterns interpretation",
      "extra": "PCA projection is like rotating a photograph to get the best view. Imagine photographing a group of people from the side - you lose depth information but capture height differences clearly. Similarly, PCA finds the 'best angles' (principal components) to view your data. The projected values tell you where each data point sits along these optimal directions. In the study hours example, the first component might represent 'overall study intensity' - students with high projected values are generally more studious across all activities."
    },
    {
      "front": "When should you prefer PCA over other dimensionality reduction techniques?",
      "back": "Use PCA when: 1) Data has linear relationships 2) You need interpretable components 3) Preserving variance is important 4) You want to remove noise while keeping signal",
      "formula": "",
      "source": "Assignment 2 - PCA Applications",
      "tags": "PCA applications dimensionality-reduction linear-relationships",
      "extra": "PCA is like a Swiss Army knife for dimensionality reduction - versatile but not always the best tool. It excels when relationships between variables are linear (like height and weight correlating). It's interpretable: you can understand what each component represents. Perfect for removing noise while keeping important signals, like cleaning up audio recordings. However, avoid PCA for non-linear relationships (use kernel PCA or autoencoders instead) or when you need to preserve specific features rather than combinations of features."
    }
  ]
}
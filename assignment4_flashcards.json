{
  "assignment4_flashcards": [
    {
      "front": "What is the closed-form solution for Ridge Regression and how does it differ from ordinary linear regression?",
      "back": "Ridge regression solution: w* = (X^T X + λI)^(-1) X^T y. The key difference is adding λI (lambda times identity matrix) to X^T X, which ensures the matrix is invertible even when X^T X is singular.",
      "formula": "\\[w^* = (X^T X + \\lambda I)^{-1} X^T y\\]",
      "source": "Assignment 4 - Ridge Regression",
      "tags": "ridge-regression regularization linear-algebra",
      "extra": "The λ parameter controls regularization strength. Higher λ = more regularization = simpler model. This prevents overfitting by penalizing large coefficients. Unlike OLS which can fail when features > samples, ridge regression always has a solution due to the λI term making the matrix invertible."
    },
    {
      "front": "Why does Ridge Regression use the L2 penalty term λ||w||₂² and what effect does it have on model coefficients?",
      "back": "The L2 penalty shrinks coefficients toward zero, preventing overfitting. It encourages smaller, more stable coefficients while keeping all features in the model (unlike L1 which can zero out features).",
      "formula": "\\[L = ||Xw - y||_2^2 + \\lambda ||w||_2^2\\]",
      "source": "Assignment 4 - Ridge Regression",
      "tags": "regularization l2-penalty overfitting",
      "extra": "Think of regularization as a 'simplicity tax' - the model pays a penalty for complexity. L2 penalty creates a circular constraint in parameter space, leading to smooth coefficient shrinkage. This is especially valuable when you have multicollinearity (correlated features) as it distributes weights more evenly across related features."
    },
    {
      "front": "What is the fundamental difference between covariance and correlation matrices, and when should each be used in PCA?",
      "back": "Covariance measures actual variance relationships but is scale-dependent. Correlation is standardized covariance (scale-independent). Use correlation matrix for PCA when features have different units/scales; use covariance when features are in same units.",
      "formula": "\\[\\text{Correlation} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\]",
      "source": "Assignment 4 - Correlation vs Covariance",
      "tags": "pca covariance correlation standardization",
      "extra": "Correlation matrix PCA is equivalent to standardizing data first, then applying covariance-based PCA. Without standardization, features with larger scales dominate the principal components. Example: If you have height (cm) and weight (kg), height's larger scale would dominate PC1 without correlation-based PCA."
    },
    {
      "front": "How does changing the scale/units of a single feature affect PCA results when using covariance vs correlation matrices?",
      "back": "Covariance-based PCA: Dramatically changes results as the rescaled feature may dominate principal components. Correlation-based PCA: No change in results as correlation standardizes all features to unit variance.",
      "formula": "\\[\\text{Standardized } X_i = \\frac{X_i - \\mu_i}{\\sigma_i}\\]",
      "source": "Assignment 4 - Scale Sensitivity",
      "tags": "pca standardization scale-invariance preprocessing",
      "extra": "This is why preprocessing matters! If you change TAX from 'per $10,000' to 'per $1', covariance-based PCA completely changes because TAX now has 10,000x larger variance. Correlation-based PCA treats all features equally regardless of their original scales, making it more robust for mixed-unit datasets."
    },
    {
      "front": "What are the minimum and maximum possible F1-scores, and what types of classifiers achieve these extremes?",
      "back": "Minimum F1 = 0 (achieved by classifier that never predicts positive class or has no correct positive predictions). Maximum F1 = 1 (achieved by perfect classifier with no false positives or false negatives).",
      "formula": "\\[F_1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]",
      "source": "Assignment 4 - F1 Score",
      "tags": "f1-score precision recall classification-metrics",
      "extra": "F1 = 0 examples: Always predict negative, or always predict positive when no positive cases exist. F1 = 1 means perfect precision AND recall simultaneously. F1 is the harmonic mean of precision and recall, so it's only high when both are high - this makes it useful for balanced evaluation of classification performance."
    },
    {
      "front": "In safety-critical applications (fire detection, mushroom identification), which confusion matrix characteristic should you prioritize and why?",
      "back": "Minimize False Negatives (maximize Recall) even at the cost of False Positives. Missing a fire or poisonous mushroom has catastrophic consequences, while false alarms are just inconvenient.",
      "formula": "\\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\]",
      "source": "Assignment 4 - Safety-Critical Classification",
      "tags": "recall safety-critical false-negatives risk-assessment",
      "extra": "This reflects the asymmetric cost of errors. Fire detection: False alarm = evacuation inconvenience; missed fire = potential deaths. Mushroom classification: False positive = skip edible mushroom; false negative = eat poison mushroom. Always consider the real-world consequences of each error type when choosing metrics and thresholds."
    },
    {
      "front": "What's the difference between micro-averaging and macro-averaging for F1-scores in multi-class classification?",
      "back": "Micro-averaging: Calculate metrics globally by counting total TP, FP, FN across all classes. Macro-averaging: Calculate metrics for each class separately, then average them. Micro-averaging favors frequent classes; macro-averaging treats all classes equally.",
      "formula": "\\[\\text{Micro-F1} = \\frac{2 \\sum TP}{2\\sum TP + \\sum FP + \\sum FN}\\]",
      "source": "Assignment 4 - Multi-class Metrics",
      "tags": "multiclass f1-score micro-averaging macro-averaging",
      "extra": "Choose based on your goal: Micro-averaging if you care about overall accuracy across all predictions (class frequency matters). Macro-averaging if you want to treat rare and common classes equally (useful for imbalanced datasets). In practice, macro-averaging often reveals problems with minority class performance that micro-averaging might hide."
    },
    {
      "front": "What does the Woodbury Matrix Identity allow us to do in the context of Ridge Regression computational complexity?",
      "back": "It provides an alternative way to compute (X^T X + λI)^(-1) that can be more efficient when n < d. Instead of inverting a d×d matrix, we can invert smaller n×n matrices, reducing complexity from O(d³) to O(n³) when beneficial.",
      "formula": "\\[(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}\\]",
      "source": "Assignment 4 - Computational Complexity",
      "tags": "woodbury-identity computational-complexity matrix-inversion",
      "extra": "This is crucial for high-dimensional data (d >> n). Standard approach: invert d×d matrix in O(d³) time. Woodbury approach: work with n×n matrices in O(n³) time. When d=10,000 and n=1,000, this is a 1000x speedup! The identity essentially lets you 'push' the inversion to the smaller dimensional space."
    }
  ]
}
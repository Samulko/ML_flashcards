{
  "assignment4_flashcards": [
    {
      "front": "What is the closed-form solution for Ridge Regression and how does it differ from ordinary linear regression?",
      "back": "Ridge regression solution: w* = (X^T X + λI)^(-1) X^T y. The key difference is adding λI (lambda times identity matrix) to X^T X, which ensures the matrix is invertible even when X^T X is singular.",
      "formula": "\\[w^* = (X^T X + \\lambda I)^{-1} X^T y\\]",
      "source": "Assignment 4 - Ridge Regression",
      "tags": "ridge-regression regularization linear-algebra",
      "extra": "ANALOGY: Like adding a small amount of noise to guarantee you can solve the equation - λI acts as a 'numerical stabilizer' that prevents matrix inversion problems.\n\nKEY INSIGHT: The λI term serves dual purposes - regularization (shrinks coefficients) and numerical stability (ensures invertibility).\n\nDIFFERENCES FROM OLS:\n• OLS: w* = (X^T X)^(-1) X^T y - can fail if X^T X is singular\n• Ridge: w* = (X^T X + λI)^(-1) X^T y - always invertible since eigenvalues get λ added\n\nTECHNICAL DETAILS:\n• λ parameter controls regularization strength\n• Higher λ = more regularization = simpler model = more bias, less variance\n• λI makes all eigenvalues positive, ensuring positive definite matrix\n\nWHEN OLS FAILS:\n• n < d (more features than samples)\n• Multicollinearity (perfectly correlated features)\n• Numerical precision issues\n\nCONNECTIONS: Related to Tikhonov regularization, bias-variance tradeoff, matrix conditioning\n\nPRACTICAL: Ridge always has a solution, making it more robust than OLS for high-dimensional or ill-conditioned problems"
    },
    {
      "front": "Why does Ridge Regression use the L2 penalty term λ||w||₂² and what effect does it have on model coefficients?",
      "back": "The L2 penalty shrinks coefficients toward zero, preventing overfitting. It encourages smaller, more stable coefficients while keeping all features in the model (unlike L1 which can zero out features).",
      "formula": "\\[L = ||Xw - y||_2^2 + \\lambda ||w||_2^2\\]",
      "source": "Assignment 4 - Ridge Regression",
      "tags": "regularization l2-penalty overfitting",
      "extra": "ANALOGY: Think of regularization as a 'simplicity tax' - the model pays a penalty for complexity.\n\nKEY INSIGHT: L2 penalty creates a circular constraint in parameter space, leading to smooth coefficient shrinkage toward zero without eliminating features entirely.\n\nTECHNICAL: The penalty term λ||w||₂² gets added to the loss function, making the optimization balance between fitting the data and keeping coefficients small.\n\nCONNECTIONS: Related to Bayesian inference (Gaussian prior), L1 vs L2 regularization differences, bias-variance tradeoff\n\nPRACTICAL: Especially valuable when you have multicollinearity (correlated features) as it distributes weights more evenly across related features."
    },
    {
      "front": "What is the fundamental difference between covariance and correlation matrices, and when should each be used in PCA?",
      "back": "Covariance measures actual variance relationships but is scale-dependent. Correlation is standardized covariance (scale-independent). Use correlation matrix for PCA when features have different units/scales; use covariance when features are in same units.",
      "formula": "\\[\\text{Correlation} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\]",
      "source": "Assignment 4 - Correlation vs Covariance",
      "tags": "pca covariance correlation standardization",
      "extra": "ANALOGY: Covariance is like comparing salaries in different currencies without exchange rates - the numbers might be meaningless. Correlation is like converting everything to the same currency first.\n\nKEY INSIGHT: Correlation matrix PCA is equivalent to standardizing data first, then applying covariance-based PCA. Choose based on whether scale differences are meaningful.\n\nTECHNICAL: Without standardization, features with larger scales dominate the principal components. Example: height (cm) vs weight (kg) - height's larger scale would dominate PC1.\n\nCONNECTIONS: Data preprocessing, standardization, feature scaling, PCA assumptions\n\nPRACTICAL: Use correlation matrix for mixed units. Use covariance when features are in same units and scale differences are meaningful."
    },
    {
      "front": "How does changing the scale/units of a single feature affect PCA results when using covariance vs correlation matrices?",
      "back": "Covariance-based PCA: Dramatically changes results as the rescaled feature may dominate principal components. Correlation-based PCA: No change in results as correlation standardizes all features to unit variance.",
      "formula": "\\[\\text{Standardized } X_i = \\frac{X_i - \\mu_i}{\\sigma_i}\\]",
      "source": "Assignment 4 - Scale Sensitivity",
      "tags": "pca standardization scale-invariance preprocessing",
      "extra": "ANALOGY: Like comparing a person's height in millimeters vs meters - the same person but completely different numbers! Covariance gets fooled by the units, correlation sees through them.\n\nKEY INSIGHT: Scale sensitivity is the Achilles heel of covariance-based PCA, while correlation-based PCA is scale-invariant.\n\nTECHNICAL: If you change TAX from 'per $10,000' to 'per $1', covariance-based PCA completely changes because TAX now has 10,000x larger variance. Correlation-based PCA treats all features equally.\n\nCONNECTIONS: Feature scaling, data preprocessing, standardization, robust statistics\n\nPRACTICAL: This is why preprocessing matters! Correlation-based PCA is more robust for mixed-unit datasets where rescaling might happen."
    },
    {
      "front": "What are the minimum and maximum possible F1-scores, and what types of classifiers achieve these extremes?",
      "back": "Minimum F1 = 0 (achieved by classifier that never predicts positive class or has no correct positive predictions). Maximum F1 = 1 (achieved by perfect classifier with no false positives or false negatives).",
      "formula": "\\[F_1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]",
      "source": "Assignment 4 - F1 Score",
      "tags": "f1-score precision recall classification-metrics",
      "extra": "ANALOGY: F1 score is like a report card that only gives you an A if you excel at both accuracy (precision) AND completeness (recall) - mediocre at both gives a mediocre score.\n\nKEY INSIGHT: F1 is the harmonic mean of precision and recall, so it's only high when both are high - perfect for balanced evaluation.\n\nTECHNICAL: F1 = 0 when classifier never predicts positive class or has no correct positives. F1 = 1 requires perfect precision AND recall simultaneously.\n\nCONNECTIONS: Related to precision-recall tradeoff, harmonic vs arithmetic mean, classification evaluation\n\nPRACTICAL: Excellent single metric for imbalanced datasets where you care about both avoiding false alarms and catching all positives."
    },
    {
      "front": "In safety-critical applications (fire detection, mushroom identification), which confusion matrix characteristic should you prioritize and why?",
      "back": "Minimize False Negatives (maximize Recall) even at the cost of False Positives. Missing a fire or poisonous mushroom has catastrophic consequences, while false alarms are just inconvenient.",
      "formula": "\\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\]",
      "source": "Assignment 4 - Safety-Critical Classification",
      "tags": "recall safety-critical false-negatives risk-assessment",
      "extra": "ANALOGY: Like a smoke detector - you'd rather have it beep unnecessarily than fail to warn you of real danger.\n\nKEY INSIGHT: In safety-critical systems, the cost of missing a positive case (false negative) far exceeds the cost of a false alarm (false positive).\n\nTECHNICAL: Fire detection: False alarm = evacuation inconvenience; missed fire = potential deaths. Mushroom classification: False positive = skip edible mushroom; false negative = eat poison mushroom.\n\nCONNECTIONS: Related to medical diagnosis, fraud detection, quality control - all domains with asymmetric error costs\n\nPRACTICAL: Always consider the real-world consequences of each error type when choosing metrics and thresholds. Set decision boundaries to minimize the most dangerous error type."
    },
    {
      "front": "What's the difference between micro-averaging and macro-averaging for F1-scores in multi-class classification?",
      "back": "Micro-averaging: Calculate metrics globally by counting total TP, FP, FN across all classes. Macro-averaging: Calculate metrics for each class separately, then average them. Micro-averaging favors frequent classes; macro-averaging treats all classes equally.",
      "formula": "\\[\\text{Micro-F1} = \\frac{2 \\sum TP}{2\\sum TP + \\sum FP + \\sum FN}\\]",
      "source": "Assignment 4 - Multi-class Metrics",
      "tags": "multiclass f1-score micro-averaging macro-averaging",
      "extra": "ANALOGY: Micro-averaging is like a popular vote (majority classes dominate), while macro-averaging is like an electoral college (each class gets equal say).\n\nKEY INSIGHT: The choice between micro and macro averaging depends on whether you want to weight classes by frequency or treat them equally.\n\nTECHNICAL: Micro-averaging calculates metrics globally by counting total TP, FP, FN across all classes. Macro-averaging calculates metrics for each class separately, then averages them.\n\nCONNECTIONS: Related to class imbalance, evaluation metrics, weighted vs unweighted averages\n\nPRACTICAL: Use macro-averaging to catch problems with minority class performance that micro-averaging might hide. Micro-averaging better reflects overall system performance."
    },
    {
      "front": "What does the Woodbury Matrix Identity allow us to do in the context of Ridge Regression computational complexity?",
      "back": "It provides an alternative way to compute (X^T X + λI)^(-1) that can be more efficient when n < d. Instead of inverting a d×d matrix, we can invert smaller n×n matrices, reducing complexity from O(d³) to O(n³) when beneficial.",
      "formula": "\\[(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}\\]",
      "source": "Assignment 4 - Computational Complexity",
      "tags": "woodbury-identity computational-complexity matrix-inversion",
      "extra": "ANALOGY: Like finding a shortcut that avoids rush hour traffic - instead of taking the slow route through high-dimensional space, you take the express lane through the smaller dimensional space.\n\nKEY INSIGHT: The Woodbury identity lets you 'push' matrix inversion to the smaller dimensional space, providing massive computational savings.\n\nTECHNICAL: Standard approach: invert d×d matrix in O(d³) time. Woodbury approach: work with n×n matrices in O(n³) time. When d=10,000 and n=1,000, this is a 1000x speedup!\n\nCONNECTIONS: Matrix calculus, Sherman-Morrison formula, computational linear algebra, kernel methods\n\nPRACTICAL: Essential for high-dimensional data where d >> n. Makes Ridge regression computationally feasible for modern datasets with thousands of features."
    }
  ]
}
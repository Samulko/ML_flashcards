{
  "assignment7_flashcards": [
    {
      "front": "What is information gain in decision trees and how is it calculated?",
      "back": "Information gain measures how much a feature reduces uncertainty (entropy) when splitting data. It's the difference between parent entropy and weighted average of children entropies.",
      "formula": "\\[IG(S,A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\\]",
      "source": "Assignment 7 - Decision Tree Splitting",
      "tags": "decision-trees information-gain entropy splitting-criteria",
      "extra": "ANALOGY: Think of information gain like organizing a messy room - each split should create more organized (less random) groups. The better the organization, the higher the information gain.\\n\\nKEY INSIGHT: Information gain quantifies how much 'surprise' or uncertainty we eliminate by learning the value of a feature. It's the reduction in entropy achieved by the split.\\n\\nTECHNICAL CALCULATION:\\n1. Calculate entropy of parent node H(S)\\n2. For each possible value v of attribute A, calculate entropy of resulting subset H(S_v)\\n3. Take weighted average of child entropies (weighted by subset size)\\n4. Information gain = parent entropy - weighted average of child entropies\\n\\nWHY WEIGHTED AVERAGE: Larger subsets contribute more to the overall entropy - reflects actual impact of the split\\n\\nGREEDY SELECTION: Decision trees choose the attribute with maximum information gain at each node\\n\\nBIAS ISSUE: Attributes with more possible values tend to have higher information gain (can create many small, pure subsets)\\n\\nCONNECTIONS: Related to mutual information, KL divergence, information theory\\n\\nPRACTICAL: In mushroom classification, the optimal first split maximizes this measure, creating the most informative initial division"
    },
    {
      "front": "What is entropy in decision trees and what does it measure?",
      "back": "Entropy measures the impurity or randomness in a dataset. Pure nodes (all same class) have entropy 0, while maximally mixed nodes approach entropy 1.",
      "formula": "\\[H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\]",
      "source": "Assignment 7 - Impurity Measures",
      "tags": "decision-trees entropy impurity-measure information-theory",
      "extra": "ANALOGY: Like measuring chaos in a group - classroom with all students paying attention (pure) has low entropy.\\n\\nKEY INSIGHT: Entropy quantifies impurity - pure nodes have entropy 0, maximally mixed nodes approach entropy 1.\\n\\nTECHNICAL: Uses logarithmic scaling, connects directly to information theory, calculated as -Î£p_i log_2(p_i).\\n\\nCONNECTIONS: Information theory, impurity measures, Gini impurity\\n\\nPRACTICAL: Decision trees aim to create pure leaf nodes by reducing entropy at each split."
    },
    {
      "front": "How do you adapt decision trees from classification to regression problems?",
      "back": "Change the splitting criterion from entropy/Gini to variance reduction, use mean (not mode) for leaf predictions, and employ MSE or MAE for evaluation instead of accuracy.",
      "formula": "\\[Variance = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2\\]",
      "source": "Assignment 7 - Regression Trees",
      "tags": "decision-trees regression classification adaptation variance-reduction",
      "extra": "ANALOGY: Like changing from sorting colored balls (discrete) to organizing by weight (continuous).\\n\\nKEY INSIGHT: Instead of 'which class appears most?' we ask 'what's the average value?' - fundamental shift from classification to regression.\\n\\nTECHNICAL: Variance replaces entropy as impurity measure, mean replaces mode for predictions, MSE/MAE replace accuracy.\\n\\nCONNECTIONS: Regression vs classification, impurity measures, continuous prediction\\n\\nPRACTICAL: Powerful for predicting house prices, temperatures, any continuous target."
    },
    {
      "front": "What are the key hyperparameters for tuning decision tree performance?",
      "back": "Max depth (tree height), max leaf nodes (terminal node count), min samples split (samples needed to split), and criterion (gini vs entropy for measuring split quality).",
      "formula": "",
      "source": "Assignment 7 - Hyperparameter Tuning", 
      "tags": "decision-trees hyperparameters overfitting model-selection cross-validation",
      "extra": "ANALOGY: Like adjusting the rules for growing a tree - max depth prevents becoming too tall (pruning).\\n\\nKEY INSIGHT: Different hyperparameters control different aspects of tree complexity and prevent overfitting.\\n\\nTECHNICAL: Max depth (tree height), min samples split (statistical significance), max leaf nodes (overall complexity), criterion choice (gini vs entropy).\\n\\nCONNECTIONS: Overfitting prevention, model complexity, cross-validation\\n\\nPRACTICAL: Use cross-validation to find sweet spot between underfitting and overfitting."
    },
    {
      "front": "What is the difference between Gini impurity and Entropy as splitting criteria?",
      "back": "Both measure node impurity, but Gini uses squared probabilities while Entropy uses logarithms. Gini is computationally faster; Entropy connects to information theory and tends to create more balanced trees.",
      "formula": "\\[Gini = 1 - \\sum_{i=1}^{c} p_i^2\\] vs \\[Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\]",
      "source": "Assignment 7 - Splitting Criteria",
      "tags": "decision-trees gini-impurity entropy splitting-criteria impurity-measures",
      "extra": "ANALOGY: Like choosing between two ways to measure messiness - Gini counts mismatched pairs, Entropy measures information content.\\n\\nKEY INSIGHT: Both measure impurity but with different mathematical approaches - squared probabilities vs logarithms.\\n\\nTECHNICAL: Gini is computationally faster, Entropy connects to information theory and creates more balanced trees.\\n\\nCONNECTIONS: Information theory, computational efficiency, splitting criteria\\n\\nPRACTICAL: Often give similar results, but Entropy preferred for information gain, Gini for speed."
    },
    {
      "front": "How do you prevent overfitting in decision trees during training?",
      "back": "Control tree complexity through pre-pruning (max depth, min samples split, max leaf nodes) or post-pruning, and use cross-validation to select optimal hyperparameters.",
      "formula": "",
      "source": "Assignment 7 - Overfitting Prevention",
      "tags": "decision-trees overfitting pruning regularization cross-validation",
      "extra": "ANALOGY: Like teaching someone to generalize rather than memorize - overfitted tree memorizes answers but can't handle new questions.\\n\\nKEY INSIGHT: Balance between capturing true patterns and avoiding noise - need just the right amount of complexity.\\n\\nTECHNICAL: Pre-pruning sets growth limits during training, post-pruning removes branches after growing.\\n\\nCONNECTIONS: Bias-variance tradeoff, generalization, model complexity\\n\\nPRACTICAL: Use cross-validation to find balance - too simple = underfit, too complex = overfit."
    },
    {
      "front": "What evaluation metrics should you use for decision tree classification and why?",
      "back": "Use precision (correct positive predictions), recall (finding all positives), F1-score (harmonic mean of precision/recall), and confusion matrix for comprehensive performance assessment.",
      "formula": "\\[Precision = \\frac{TP}{TP + FP}\\], \\[Recall = \\frac{TP}{TP + FN}\\], \\[F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\]",
      "source": "Assignment 7 - Model Evaluation",
      "tags": "decision-trees evaluation-metrics precision recall f1-score classification-performance",
      "extra": "ANALOGY: Like medical diagnosis - high precision = few false alarms, high recall = catching all sick patients.\\n\\nKEY INSIGHT: Accuracy alone misleading with imbalanced data - need precision, recall, and F1-score for complete picture.\\n\\nTECHNICAL: Precision = correct positive predictions, Recall = finding all positives, F1 = harmonic mean of both.\\n\\nCONNECTIONS: Imbalanced datasets, classification metrics, confusion matrix\\n\\nPRACTICAL: Use confusion matrix to see exactly where your tree makes mistakes."
    }
  ]
}
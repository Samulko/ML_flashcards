{
  "assignment3_flashcards": [
    {
      "front": "What is the Normal Matrix in linear regression and what are its key mathematical properties?",
      "back": "The Normal Matrix is X^T X that appears in the linear regression solution. It is symmetric and positive semi-definite.",
      "formula": "\\[\\text{Normal Matrix} = X^T X \\in \\mathbb{R}^{d \\times d}\\]",
      "source": "Assignment 3 - Normal Matrix Properties",
      "tags": "linear-regression normal-matrix matrix-properties",
      "extra": "ANALOGY: Think of it as the 'fingerprint' of your data's structure - encodes all relationships between features in a compact matrix form.\n\nKEY INSIGHT: The Normal Matrix is the Gram matrix of your feature vectors - it measures how similar features are to each other.\n\nMATHEMATICAL PROPERTIES:\n• Symmetric: X^T X = (X^T X)^T - matrix equals its transpose\n• Positive semi-definite: all eigenvalues ≥ 0 (v^T (X^T X) v = ||Xv||² ≥ 0)\n• Size: d×d where d is number of features (not number of samples)\n\nTECHNICAL INTERPRETATION:\n• Diagonal entries: ||x_j||² (squared norm of each feature)\n• Off-diagonal entries: x_i^T x_j (dot product between features i and j)\n• Encodes feature correlations and multicollinearity\n\nPRACTICAL IMPORTANCE:\n• Must be invertible for unique solution\n• Condition number indicates numerical stability\n• Large condition number → multicollinearity problems\n\nCONNECTIONS: Related to covariance matrix, Gram matrix, feature correlation matrix\n\nWHY IT MATTERS: Determines if regression problem is well-posed and solvable"
    },
    {
      "front": "When does linear regression have a unique solution?",
      "back": "When the Normal Matrix X^T X is positive definite (all eigenvalues > 0), which requires linearly independent columns in X.",
      "formula": "\\[\\hat{w} = (X^T X)^{-1} X^T y \\text{ exists uniquely when } X^T X \\text{ is invertible}\\]",
      "source": "Assignment 3 - Unique Solutions",
      "tags": "linear-regression uniqueness invertibility",
      "extra": "ANALOGY: Imagine trying to find the intersection of lines. If features are linearly dependent, it's like having parallel lines - no unique intersection. Independent features give you lines that meet at exactly one point.\n\nKEY INSIGHT: Uniqueness requires full column rank of X - no feature can be written as combination of other features.\n\nMATHEMATICAL CONDITIONS:\n• X^T X must be positive definite (not just semi-definite)\n• All eigenvalues of X^T X must be > 0 (not just ≥ 0)\n• X must have linearly independent columns\n• Usually requires n > d (more samples than features)\n\nWHAT GOES WRONG:\n• Multicollinearity: features are correlated → X^T X is singular\n• Perfect correlation: one feature = linear combination of others\n• Insufficient data: n ≤ d leads to underdetermined system\n\nTECHNICAL SOLUTIONS:\n• Regularization (Ridge): adds λI to X^T X, ensures invertibility\n• Feature selection: remove redundant features\n• Dimensionality reduction: PCA before regression\n\nCONNECTIONS: Related to rank, condition number, multicollinearity\n\nPRACTICAL: Check condition number of X^T X - values > 10^12 indicate numerical problems"
    },
    {
      "front": "What are feature maps and how do they extend linear regression capabilities?",
      "back": "Feature maps Φ(X) transform input data into higher-dimensional space, allowing linear models to capture non-linear patterns.",
      "formula": "\\[\\Phi: \\mathbb{R}^d \\to \\mathbb{R}^D, \\quad \\hat{y} = \\Phi(X)w\\]",
      "source": "Assignment 3 - Feature Maps",
      "tags": "feature-maps non-linear-regression polynomial-features",
      "extra": "ANALOGY: Like putting on special glasses that reveal hidden patterns - the same data looks different and more structured in the transformed space.\n\nKEY INSIGHT: Instead of changing the algorithm, change the data representation. Linear regression in feature space can capture non-linear patterns in original space.\n\nCOMMON FEATURE MAPS:\n• Polynomial: x → [1, x, x², x³, ...] - fits curves with linear model\n• RBF: x → [exp(-||x-μ₁||²), exp(-||x-μ₂||²), ...] - creates local bumps\n• Piecewise: x → [I(x∈[a₁,b₁]), I(x∈[a₂,b₂]), ...] - step functions\n• Fourier: x → [sin(ωx), cos(ωx), sin(2ωx), cos(2ωx), ...] - periodic patterns\n\nTECHNICAL PROCESS:\n1. Choose appropriate feature map Φ\n2. Transform training data: X → Φ(X)\n3. Solve linear regression in feature space\n4. Predictions: ŷ = Φ(x_new)w\n\nTRADE-OFFS:\n• Higher dimensions → more expressive but risk overfitting\n• Computational cost increases with D\n• Need to choose right feature map for data structure\n\nCONNECTIONS: Related to kernel methods, basis functions, neural networks (hidden layers)\n\nPRACTICAL: Make data linearly separable in higher dimension rather than wrestling with non-linearity directly"
    },
    {
      "front": "How do polynomial features work and what's the trade-off with degree?",
      "back": "Polynomial features create powers of input: [x, x², x³, ...]. Higher degrees fit training data better but risk overfitting.",
      "formula": "\\[\\Phi_{poly}(x) = [1, x, x^2, x^3, \\ldots, x^d]\\]",
      "source": "Assignment 3 - Polynomial Features",
      "tags": "polynomial-features overfitting model-complexity",
      "extra": "Think of polynomial degree as 'flexibility knob'. Low degree = rigid ruler (underfitting), high degree = bendy snake (overfitting). Degree 1 fits lines, degree 2 parabolas, degree 3 S-curves. The sweet spot balances expressiveness with generalization. Watch training vs validation error: when they diverge, you've gone too far."
    },
    {
      "front": "What are Radial Basis Function (RBF) features and when are they useful?",
      "back": "RBF features create localized 'bumps' centered at specific positions, useful for capturing local patterns in data.",
      "formula": "\\[\\Phi_{RBF}(x) = \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\]",
      "source": "Assignment 3 - RBF Features",
      "tags": "rbf-features gaussian-basis local-features",
      "extra": "Imagine placing 'hills' of influence at key data points. Each RBF is like a spotlight with Gaussian falloff - bright at center, dim at edges. Scale σ controls width: small σ = sharp peaks (local fit), large σ = broad hills (smooth fit). Perfect for data with local structure, like modeling city temperatures where nearby locations are similar."
    },
    {
      "front": "What computational challenges arise with high-dimensional linear regression?",
      "back": "Memory requirements grow quadratically (O(d²)) for storing X^T X, making matrix inversion computationally prohibitive.",
      "formula": "\\[\\text{Memory for } X^T X = d^2 \\times 4 \\text{ bytes (float32)}\\]",
      "source": "Assignment 3 - Computational Challenges",
      "tags": "computational-complexity memory-requirements high-dimensional",
      "extra": "Real example: 512×512×3 images = 786,432 features. X^T X needs 2.3 TB memory! Even supercomputers struggle. Solutions: (1) Gradient descent avoids computing X^T X, (2) Dimensionality reduction via PCA, (3) Regularization prevents overfitting, (4) Online learning processes data in chunks. The curse of dimensionality is real - plan accordingly."
    },
    {
      "front": "How do piecewise features work and what patterns do they capture?",
      "back": "Piecewise features divide input space into regions. Constant creates step functions, linear creates connected line segments.",
      "formula": "\\[\\Phi_{piece}(x) = \\begin{cases} 1 & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\end{cases}\\]",
      "source": "Assignment 3 - Piecewise Features",
      "tags": "piecewise-features step-functions local-models",
      "extra": "Think of building a staircase approximation to curved data. Piecewise constant = stairs with flat steps, piecewise linear = connecting the steps with ramps. Great for data with distinct regimes (like different price brackets) or when you suspect the relationship changes at certain thresholds. Each piece models a local linear relationship."
    },
    {
      "front": "What is RMSE and why is it important for regression evaluation?",
      "back": "Root Mean Square Error measures average prediction error in original units. Lower RMSE indicates better model performance.",
      "formula": "\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]",
      "source": "Assignment 3 - Model Evaluation",
      "tags": "rmse evaluation-metrics regression-performance",
      "extra": "RMSE is like a 'average wrongness' meter in original units. If predicting house prices, RMSE = $10k means you're typically off by $10k. It penalizes large errors more than small ones (squared term), so it's sensitive to outliers. Compare training vs test RMSE: similar values = good generalization, large gap = overfitting. Target: RMSE < 4.0 for Boston housing is a solid benchmark."
    }
  ]
}
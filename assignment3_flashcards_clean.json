{
  "assignment3_flashcards": [
    {
      "front": "What is the Normal Matrix in linear regression and what are its key mathematical properties?",
      "back": "The Normal Matrix is X^T X that appears in the linear regression solution. It is symmetric and positive semi-definite.",
      "formula": "\\[\\text{Normal Matrix} = X^T X \\in \\mathbb{R}^{d \\times d}\\]",
      "source": "Assignment 3 - Normal Matrix Properties",
      "tags": "linear-regression normal-matrix matrix-properties",
      "extra": "ANALOGY: Think of it as the 'fingerprint' of your data's structure - encodes all relationships between features in a compact matrix form.\\n\\nKEY INSIGHT: The Normal Matrix is the Gram matrix of your feature vectors - it measures how similar features are to each other.\\n\\nMATHEMATICAL PROPERTIES:\\n• Symmetric: X^T X = (X^T X)^T - matrix equals its transpose\\n• Positive semi-definite: all eigenvalues ≥ 0 (v^T (X^T X) v = ||Xv||² ≥ 0)\\n• Size: d×d where d is number of features (not number of samples)\\n\\nTECHNICAL INTERPRETATION:\\n• Diagonal entries: ||x_j||² (squared norm of each feature)\\n• Off-diagonal entries: x_i^T x_j (dot product between features i and j)\\n• Encodes feature correlations and multicollinearity\\n\\nPRACTICAL IMPORTANCE:\\n• Must be invertible for unique solution\\n• Condition number indicates numerical stability\\n• Large condition number → multicollinearity problems\\n\\nCONNECTIONS: Related to covariance matrix, Gram matrix, feature correlation matrix\\n\\nWHY IT MATTERS: Determines if regression problem is well-posed and solvable"
    },
    {
      "front": "When does linear regression have a unique solution?",
      "back": "When the Normal Matrix X^T X is positive definite (all eigenvalues > 0), which requires linearly independent columns in X.",
      "formula": "\\[\\hat{w} = (X^T X)^{-1} X^T y \\text{ exists uniquely when } X^T X \\text{ is invertible}\\]",
      "source": "Assignment 3 - Unique Solutions",
      "tags": "linear-regression uniqueness invertibility",
      "extra": "ANALOGY: Imagine trying to find the intersection of lines. If features are linearly dependent, it's like having parallel lines - no unique intersection. Independent features give you lines that meet at exactly one point.\\n\\nKEY INSIGHT: Uniqueness requires full column rank of X - no feature can be written as combination of other features.\\n\\nMATHEMATICAL CONDITIONS:\\n• X^T X must be positive definite (not just semi-definite)\\n• All eigenvalues of X^T X must be > 0 (not just ≥ 0)\\n• X must have linearly independent columns\\n• Usually requires n > d (more samples than features)\\n\\nWHAT GOES WRONG:\\n• Multicollinearity: features are correlated → X^T X is singular\\n• Perfect correlation: one feature = linear combination of others\\n• Insufficient data: n ≤ d leads to underdetermined system\\n\\nTECHNICAL SOLUTIONS:\\n• Regularization (Ridge): adds λI to X^T X, ensures invertibility\\n• Feature selection: remove redundant features\\n• Dimensionality reduction: PCA before regression\\n\\nCONNECTIONS: Related to rank, condition number, multicollinearity\\n\\nPRACTICAL: Check condition number of X^T X - values > 10^12 indicate numerical problems"
    },
    {
      "front": "What are feature maps and how do they extend linear regression capabilities?",
      "back": "Feature maps Φ(X) transform input data into higher-dimensional space, allowing linear models to capture non-linear patterns.",
      "formula": "\\[\\Phi: \\mathbb{R}^d \\to \\mathbb{R}^D, \\quad \\hat{y} = \\Phi(X)w\\]",
      "source": "Assignment 3 - Feature Maps",
      "tags": "feature-maps non-linear-regression polynomial-features",
      "extra": "ANALOGY: Like putting on special glasses that reveal hidden patterns - the same data looks different and more structured in the transformed space.\\n\\nKEY INSIGHT: Instead of changing the algorithm, change the data representation. Linear regression in feature space can capture non-linear patterns in original space.\\n\\nCOMMON FEATURE MAPS:\\n• Polynomial: x → [1, x, x², x³, ...] - fits curves with linear model\\n• RBF: x → [exp(-||x-μ₁||²), exp(-||x-μ₂||²), ...] - creates local bumps\\n• Piecewise: x → [I(x∈[a₁,b₁]), I(x∈[a₂,b₂]), ...] - step functions\\n• Fourier: x → [sin(ωx), cos(ωx), sin(2ωx), cos(2ωx), ...] - periodic patterns\\n\\nTECHNICAL PROCESS:\\n1. Choose appropriate feature map Φ\\n2. Transform training data: X → Φ(X)\\n3. Solve linear regression in feature space\\n4. Predictions: ŷ = Φ(x_new)w\\n\\nTRADE-OFFS:\\n• Higher dimensions → more expressive but risk overfitting\\n• Computational cost increases with D\\n• Need to choose right feature map for data structure\\n\\nCONNECTIONS: Related to kernel methods, basis functions, neural networks (hidden layers)\\n\\nPRACTICAL: Make data linearly separable in higher dimension rather than wrestling with non-linearity directly"
    },
    {
      "front": "How do polynomial features work and what's the trade-off with degree?",
      "back": "Polynomial features create powers of input: [x, x², x³, ...]. Higher degrees fit training data better but risk overfitting.",
      "formula": "\\[\\Phi_{poly}(x) = [1, x, x^2, x^3, \\ldots, x^d]\\]",
      "source": "Assignment 3 - Polynomial Features",
      "tags": "polynomial-features overfitting model-complexity",
      "extra": "ANALOGY: Think of polynomial degree as 'flexibility knob' - low degree = rigid ruler, high degree = bendy snake.\\n\\nKEY INSIGHT: The sweet spot balances expressiveness with generalization - watch for training/validation error divergence.\\n\\nTECHNICAL: Degree 1 fits lines, degree 2 parabolas, degree 3 S-curves. Higher degrees fit training data better but risk overfitting.\\n\\nCONNECTIONS: Bias-variance tradeoff, model complexity, regularization\\n\\nPRACTICAL: Use cross-validation to find optimal degree that generalizes well."
    },
    {
      "front": "What are Radial Basis Function (RBF) features and when are they useful?",
      "back": "RBF features create localized 'bumps' centered at specific positions, useful for capturing local patterns in data.",
      "formula": "\\[\\Phi_{RBF}(x) = \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\]",
      "source": "Assignment 3 - RBF Features",
      "tags": "rbf-features gaussian-basis local-features",
      "extra": "ANALOGY: Like placing 'hills' of influence at key data points - each RBF is a spotlight with Gaussian falloff.\\n\\nKEY INSIGHT: Scale σ controls locality - small σ = sharp peaks (local fit), large σ = broad hills (smooth fit).\\n\\nTECHNICAL: Creates localized 'bumps' centered at specific positions using Gaussian functions.\\n\\nCONNECTIONS: Gaussian functions, kernel methods, support vector machines\\n\\nPRACTICAL: Perfect for data with local structure, like modeling city temperatures where nearby locations are similar."
    },
    {
      "front": "What computational challenges arise with high-dimensional linear regression?",
      "back": "Memory requirements grow quadratically (O(d²)) for storing X^T X, making matrix inversion computationally prohibitive.",
      "formula": "\\[\\text{Memory for } X^T X = d^2 \\times 4 \\text{ bytes (float32)}\\]",
      "source": "Assignment 3 - Computational Challenges",
      "tags": "computational-complexity memory-requirements high-dimensional",
      "extra": "ANALOGY: Like trying to store every possible relationship in a massive phone book - it quickly becomes unmanageable.\\n\\nKEY INSIGHT: Memory grows quadratically O(d²) - the curse of dimensionality hits hard in practice.\\n\\nTECHNICAL: Real example - 512×512×3 images = 786,432 features, X^T X needs 2.3 TB memory!\\n\\nCONNECTIONS: Curse of dimensionality, computational complexity, matrix operations\\n\\nPRACTICAL: Use gradient descent, PCA, regularization, or online learning to avoid computing full X^T X."
    },
    {
      "front": "How do piecewise features work and what patterns do they capture?",
      "back": "Piecewise features divide input space into regions. Constant creates step functions, linear creates connected line segments.",
      "formula": "\\[\\Phi_{piece}(x) = \\begin{cases} 1 & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\end{cases}\\]",
      "source": "Assignment 3 - Piecewise Features",
      "tags": "piecewise-features step-functions local-models",
      "extra": "ANALOGY: Like building a staircase approximation to curved data - piecewise constant = flat steps, piecewise linear = connected ramps.\\n\\nKEY INSIGHT: Each piece models a local linear relationship within its region - perfect for data with distinct regimes.\\n\\nTECHNICAL: Divides input space into regions with different behaviors (step functions or linear segments).\\n\\nCONNECTIONS: Decision trees, regression trees, local models\\n\\nPRACTICAL: Great for data with thresholds (price brackets) or regime changes."
    },
    {
      "front": "What is RMSE and why is it important for regression evaluation?",
      "back": "Root Mean Square Error measures average prediction error in original units. Lower RMSE indicates better model performance.",
      "formula": "\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]",
      "source": "Assignment 3 - Model Evaluation",
      "tags": "rmse evaluation-metrics regression-performance",
      "extra": "ANALOGY: Like an 'average wrongness' meter in original units - RMSE = $10k means typically off by $10k.\\n\\nKEY INSIGHT: Penalizes large errors more than small ones due to squared term - sensitive to outliers.\\n\\nTECHNICAL: Root Mean Square Error measures average prediction error, lower values indicate better performance.\\n\\nCONNECTIONS: Mean squared error, model evaluation, loss functions\\n\\nPRACTICAL: Compare training vs test RMSE - similar values = good generalization, large gap = overfitting."
    }
  ]
}
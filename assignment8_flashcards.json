{
  "assignment8_flashcards": [
    {
      "front": "What is the variance formula for Random Forest predictor and what does it tell us about ensemble benefits?",
      "back": "Random Forest variance = (ρ + (1-ρ)/B) × σ², where ρ is tree correlation, B is number of trees, σ² is individual tree variance",
      "formula": "\\[\\text{Var}(\\hat{f}_B) = \\left(\\rho + \\frac{1-\\rho}{B}\\right)\\sigma^2\\]",
      "source": "Assignment 8 - Random Forest Theory",
      "tags": "random-forest variance ensemble correlation",
      "extra": "ANALOGY: Like asking multiple weather forecasters - if they all use the same data sources (high correlation ρ), their combined prediction isn't much better than one forecaster. But if they use diverse sources (low ρ), averaging reduces errors significantly.\\n\\nKEY INSIGHT: This formula reveals the fundamental trade-off in ensemble methods - you need both multiple models (large B) AND diversity between them (low ρ) for maximum benefit.\\n\\nTECHNICAL BREAKDOWN:\\n• ρσ²: Irreducible variance floor (correlation × individual variance)\\n• (1-ρ)σ²/B: Reducible variance that decreases with more trees\\n• As B → ∞, variance approaches ρσ² (correlation sets the limit)\\n\\nWHY RANDOM FORESTS WORK:\\n• Bagging alone (same features): moderate ρ reduction\\n• Feature randomization: further reduces ρ by preventing dominant features\\n• Bootstrap sampling: adds diversity through different training sets\\n\\nPRACTICAL IMPLICATIONS:\\n• More trees help up to a point (diminishing returns)\\n• Reducing correlation ρ is often more important than adding trees\\n• Feature randomization is crucial for effectiveness\\n\\nCONNECTIONS: Related to bias-variance decomposition, central limit theorem, portfolio theory\\n\\nLIMITATION: Even infinite perfectly diverse trees can't reduce variance below ρσ²"
    },
    {
      "front": "How does Random Forest reduce correlation between trees and why is this crucial?",
      "back": "Random Forest selects random subset of features at each split, decorrelating trees by preventing dominant features from being used consistently across all trees",
      "formula": "",
      "source": "Assignment 8 - Random Forest",
      "tags": "random-forest decorrelation feature-selection ensemble",
      "extra": "ANALOGY: Like asking different experts who each specialize in different areas - combined wisdom is more reliable when experts consider different aspects.\\n\\nKEY INSIGHT: Feature randomization breaks correlation between trees, which is crucial for ensemble effectiveness.\\n\\nTECHNICAL: Random Forest achieves this by randomly restricting which features each tree can consider at each split.\\n\\nCONNECTIONS: Ensemble methods, bias-variance tradeoff, feature selection\\n\\nPRACTICAL: Prevents dominant features from being used consistently across all trees."
    },
    {
      "front": "What is a decision stump and why is it the preferred weak learner for AdaBoost?",
      "back": "A decision stump is a decision tree with maximum depth 1 (single split). It's ideal for AdaBoost because it's simple, fast to train, and provides the 'weak learning' property",
      "formula": "",
      "source": "Assignment 8 - AdaBoost",
      "tags": "adaboost decision-stump weak-learner boosting",
      "extra": "ANALOGY: Like using simple yes/no questions instead of complex essays - easier to grade and combine multiple perspectives.\\n\\nKEY INSIGHT: Simplicity is a feature, not a bug - stumps provide the 'weak learning' property essential for boosting.\\n\\nTECHNICAL: Decision stumps make decisions based on just one feature and one threshold, satisfying weak learning assumption.\\n\\nCONNECTIONS: Boosting theory, weak learning, ensemble diversity\\n\\nPRACTICAL: Train quickly, rarely overfit, provide diverse perspectives when combined."
    },
    {
      "front": "How does AdaBoost calculate the weight α_m for each weak learner and what does it represent?",
      "back": "α_m = (1/2) × ln((1-ε_m)/ε_m), where ε_m is the weighted error rate. Higher α means more influence in final ensemble",
      "formula": "\\[\\alpha_m = \\frac{1}{2}\\ln\\left(\\frac{1-\\varepsilon_m}{\\varepsilon_m}\\right)\\]",
      "source": "Assignment 8 - AdaBoost",
      "tags": "adaboost alpha-weight ensemble weighted-voting",
      "extra": "ANALOGY: Like a democratic voting system where experts with better track records get louder voices.\\n\\nKEY INSIGHT: The formula creates exponential weighting - better classifiers get dramatically more influence.\\n\\nTECHNICAL: When ε→0, α→∞ (perfect dominance). When ε=0.5, α=0 (no influence). When ε>0.5, α<0 (flip predictions).\\n\\nCONNECTIONS: Weighted voting, exponential functions, ensemble theory\\n\\nPRACTICAL: Natural quality-based voting system where good classifiers have strong voices."
    },
    {
      "front": "How does AdaBoost update sample weights and why does this improve learning?",
      "back": "Weights are updated as w_i^(m+1) = w_i^(m) × exp(-α_m × y_i × G_m(x_i)). Misclassified samples get increased weights, correctly classified get decreased weights",
      "formula": "\\[w_i^{(m+1)} = w_i^{(m)} \\cdot \\exp(-\\alpha_m y_i G_m(x_i))\\]",
      "source": "Assignment 8 - AdaBoost",
      "tags": "adaboost weight-update adaptive-learning sequential",
      "extra": "ANALOGY: Like a teacher focusing more attention on struggling students - next lesson emphasizes what students got wrong.\\n\\nKEY INSIGHT: Sequential learning where each classifier specializes in fixing previous mistakes.\\n\\nTECHNICAL: Correct predictions decrease weights, incorrect predictions increase weights exponentially.\\n\\nCONNECTIONS: Adaptive learning, sequential optimization, error correction\\n\\nPRACTICAL: Forces next learner to focus on 'hard' examples that previous learners missed."
    },
    {
      "front": "What is the key difference between Random Forest (bagging) and AdaBoost (boosting) learning strategies?",
      "back": "Random Forest trains trees independently in parallel on different data subsets. AdaBoost trains learners sequentially, where each focuses on mistakes of previous learners",
      "formula": "",
      "source": "Assignment 8 - Ensemble Comparison",
      "tags": "ensemble bagging boosting parallel sequential",
      "extra": "ANALOGY: Random Forest = independent committee voting equally. AdaBoost = relay team where each fixes previous mistakes.\\n\\nKEY INSIGHT: Different strategies - RF reduces variance through averaging, AdaBoost reduces bias by focusing on hard cases.\\n\\nTECHNICAL: RF trains in parallel, AdaBoost trains sequentially. RF more robust to noise, AdaBoost may overfit.\\n\\nCONNECTIONS: Bias-variance tradeoff, parallel vs sequential learning\\n\\nPRACTICAL: Choose RF for robustness, AdaBoost for lower training error."
    },
    {
      "front": "What does 'polarity' mean in the AdaBoost decision stump context and how does it affect classification?",
      "back": "Polarity determines which side of the threshold gets +1 label. Polarity=+1: left/below threshold → +1. Polarity=-1: left/below threshold → -1",
      "formula": "",
      "source": "Assignment 8 - AdaBoost Implementation",
      "tags": "adaboost decision-stump polarity threshold classification",
      "extra": "ANALOGY: Like choosing which way to orient a decision boundary - which side gets the positive label.\\n\\nKEY INSIGHT: Polarity flexibility allows each stump to find the best orientation for separating classes.\\n\\nTECHNICAL: Polarity=+1 means left/below → +1. Polarity=-1 means left/below → -1.\\n\\nCONNECTIONS: Decision boundaries, binary classification, threshold optimization\\n\\nPRACTICAL: Essential for finding optimal splits in decision stumps."
    },
    {
      "front": "Why might different initial stumps in AdaBoost lead to different final ensemble classifiers?",
      "back": "Each initial stump creates different weight distributions, causing subsequent learners to focus on different misclassified examples, leading to distinct learning paths and decision boundaries",
      "formula": "",
      "source": "Assignment 8 - AdaBoost Paths",
      "tags": "adaboost initialization path-dependence ensemble diversity",
      "extra": "ANALOGY: Like a snowball effect - first choice determines which mistakes get emphasized, shaping all subsequent learning.\\n\\nKEY INSIGHT: Small initial differences create dramatically different final classifiers through path dependence.\\n\\nTECHNICAL: Different stumps misclassify different points, creating distinct weight distributions and learning paths.\\n\\nCONNECTIONS: Butterfly effect, path dependence, ensemble diversity\\n\\nPRACTICAL: AdaBoost can find multiple valid solutions with different strengths."
    }
  ]
}
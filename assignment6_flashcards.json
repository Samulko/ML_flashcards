{
  "assignment6_flashcards": [
    {
      "front": "What is the softmax function and why is it used in multiclass logistic regression?",
      "back": "The softmax function converts raw logits into normalized class probabilities that sum to 1",
      "formula": "\\[\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\\]",
      "source": "Assignment 6 - Logistic Regression",
      "tags": "logistic-regression softmax multiclass probability",
      "extra": "ANALOGY: Think of softmax as a 'soft' version of argmax - like a talent competition where the best performer gets the highest score, but everyone gets some points based on their relative performance.\\n\\nKEY INSIGHT: Softmax is a generalization of the sigmoid function to multiple classes - it squashes any real-valued vector into a probability distribution.\\n\\nTECHNICAL PROPERTIES:\\n• Exponential function ensures all outputs are positive\\n• Normalization ensures probabilities sum to 1: Σ p_i = 1\\n• Differentiable everywhere (unlike argmax)\\n• Amplifies differences between logits due to exponential\\n\\nWHY EXPONENTIAL:\\n• Creates clear winner (highest logit gets disproportionately high probability)\\n• Maintains ordering of logits\\n• Mathematical convenience for gradients\\n\\nVS OTHER FUNCTIONS:\\n• Sigmoid: binary classification only\\n• Argmax: non-differentiable, no gradients\\n• Linear normalization: doesn't emphasize differences\\n\\nCONNECTIONS: Related to Boltzmann distribution, maximum entropy, cross-entropy loss\\n\\nPRACTICAL: Essential for multiclass classification where you need interpretable probability distributions and gradient-based optimization"
    },
    {
      "front": "What is the Bayes decision rule for optimal classification?",
      "back": "Assign observation x to class with highest posterior probability: argmax P(class|x)",
      "formula": "\\[\\hat{y} = \\arg\\max_k P(C_k|\\mathbf{x}) = \\arg\\max_k P(\\mathbf{x}|C_k)P(C_k)\\]",
      "source": "Assignment 6 - Bayes Error",
      "tags": "bayes-rule optimal-classifier decision-theory",
      "extra": "ANALOGY: Like having perfect knowledge of the true data distribution - if you knew exactly how nature generates data, this is how you'd classify optimally.\\n\\nKEY INSIGHT: The Bayes classifier minimizes the probability of misclassification by using all available information optimally. No other classifier can achieve lower error rate.\\n\\nTECHNICAL BREAKDOWN:\\n• P(C_k|x): Posterior probability (what we want)\\n• P(x|C_k): Likelihood (how likely x is given class k)\\n• P(C_k): Prior probability (base rate of class k)\\n• Uses Bayes' theorem: P(C|x) = P(x|C)P(C)/P(x)\\n\\nDECISION PROCESS:\\n1. Compute posterior for each class\\n2. Select class with maximum posterior\\n3. Decision boundary occurs where posteriors are equal\\n\\nPRACTICAL CHALLENGES:\\n• True distributions usually unknown\\n• Must estimate P(x|C_k) and P(C_k) from data\\n• Computational complexity for high-dimensional x\\n\\nCONNECTIONS: Foundation for Naive Bayes, LDA, QDA, and many other classifiers\\n\\nGOLD STANDARD: Provides theoretical benchmark for comparing real classifiers - any classifier with error significantly above Bayes error has room for improvement"
    },
    {
      "front": "What is the Bayes error and what does it represent?",
      "back": "The minimum possible classification error rate achieved by the optimal Bayes classifier",
      "formula": "\\[\\epsilon_{Bayes} = \\int 1 - \\max_k P(C_k|\\mathbf{x}) \\, d\\mathbf{x}\\]",
      "source": "Assignment 6 - Bayes Error",
      "tags": "bayes-error optimal-performance theoretical-limit",
      "extra": "ANALOGY: Like the 'speed of light' for classification - a fundamental limit you cannot exceed.\\n\\nKEY INSIGHT: Represents irreducible error due to overlapping class distributions - the best any classifier can possibly do.\\n\\nTECHNICAL: High Bayes error means classes are inherently hard to separate, low Bayes error means clear separation exists.\\n\\nCONNECTIONS: Optimal classification, theoretical limits, model evaluation\\n\\nPRACTICAL: Any classifier with error significantly above Bayes error has room for improvement."
    },
    {
      "front": "How does cross-entropy loss work in logistic regression and why is it preferred over MSE?",
      "back": "Cross-entropy measures difference between predicted and true probability distributions, providing better gradients for classification",
      "formula": "\\[L = -\\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{p}_{ik})\\]",
      "source": "Assignment 6 - Logistic Regression",
      "tags": "cross-entropy loss-function classification optimization",
      "extra": "ANALOGY: Like asking 'how surprised am I by the true answer given my predictions?' - perfect predictions = zero surprise.\\n\\nKEY INSIGHT: Cross-entropy provides strong gradients even when predictions are very wrong, leading to faster convergence than MSE.\\n\\nTECHNICAL: Measures difference between predicted and true probability distributions, penalizing confident wrong predictions heavily.\\n\\nCONNECTIONS: Information theory, maximum likelihood, gradient-based optimization\\n\\nPRACTICAL: Preferred over MSE for classification because it avoids gradient saturation."
    },
    {
      "front": "Why is linear regression generally unsuitable for classification tasks?",
      "back": "Linear regression outputs unbounded continuous values, lacks probabilistic interpretation, and uses inappropriate loss function (MSE) for discrete targets",
      "formula": "\\[\\text{Linear: } \\hat{y} = \\mathbf{w}^T\\mathbf{x} + b \\text{ (unbounded)}\\]",
      "source": "Assignment 6 - Linear vs Logistic",
      "tags": "linear-regression classification limitations model-choice",
      "extra": "ANALOGY: Like using a thermometer to measure personality types - wrong tool for the job!\\n\\nKEY INSIGHT: Linear regression treats class labels as continuous numbers, implying meaningless ordinal relationships between classes.\\n\\nTECHNICAL: Outputs unbounded values, lacks probabilistic interpretation, uses inappropriate MSE loss for discrete targets.\\n\\nCONNECTIONS: Model assumptions, loss functions, probability theory\\n\\nPRACTICAL: Use logistic regression for classification - it's designed for discrete outcomes."
    },
    {
      "front": "How do class priors affect the Bayes decision boundary?",
      "back": "Higher prior probability shifts decision boundary toward the less likely class, reducing its decision region",
      "formula": "\\[P(C_k|\\mathbf{x}) \\propto P(\\mathbf{x}|C_k)P(C_k)\\]",
      "source": "Assignment 6 - Bayes Error",
      "tags": "class-priors decision-boundary bayes-rule",
      "extra": "ANALOGY: Like 'voting weights' - if one class is common, you need stronger evidence to predict the rare class.\\n\\nKEY INSIGHT: Higher prior probability shifts decision boundary toward the less likely class, reducing its decision region.\\n\\nTECHNICAL: Increasing P(B) makes decision boundary shift away from Class B's center, shrinking B's decision region.\\n\\nCONNECTIONS: Bayesian inference, medical diagnosis, asymmetric costs\\n\\nPRACTICAL: Reflects real-world class frequencies and misclassification costs."
    },
    {
      "front": "What happens to decision boundaries when classes have different covariance matrices in Gaussian classification?",
      "back": "Decision boundaries become non-linear (quadratic) curves instead of straight lines",
      "formula": "\\[\\Sigma_A = I, \\Sigma_B = 2I \\Rightarrow \\text{quadratic boundary}\\]",
      "source": "Assignment 6 - Bayes Error",
      "tags": "gaussian-classification covariance decision-boundary quadratic",
      "extra": "ANALOGY: Equal covariances = parallel parking spaces (linear), different covariances = parking around circular building (curved).\\n\\nKEY INSIGHT: Different covariance matrices create quadratic decision boundaries due to Mahalanobis distance terms.\\n\\nTECHNICAL: When one class is more spread out, its influence extends further but with lower density, creating curved boundaries.\\n\\nCONNECTIONS: Quadratic discriminant analysis, Mahalanobis distance, multivariate Gaussians\\n\\nPRACTICAL: Creates elliptical or hyperbolic decision regions rather than straight lines."
    },
    {
      "front": "How does gradient descent work in logistic regression training?",
      "back": "Iteratively updates weights by moving in direction of negative gradient to minimize cross-entropy loss",
      "formula": "\\[\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla L(\\mathbf{w})\\]",
      "source": "Assignment 6 - Logistic Regression",
      "tags": "gradient-descent optimization training logistic-regression",
      "extra": "ANALOGY: Like rolling a ball down a hill to find the bottom - follows steepest downward slope at each step.\\n\\nKEY INSIGHT: Gradient points toward steepest increase, so negative gradient points toward steepest decrease in loss.\\n\\nTECHNICAL: Learning rate α controls step size - too large causes overshooting, too small causes slow convergence.\\n\\nCONNECTIONS: Optimization theory, numerical methods, machine learning training\\n\\nPRACTICAL: Required for logistic regression due to non-linear sigmoid/softmax (no closed-form solution like linear regression)."
    }
  ]
}
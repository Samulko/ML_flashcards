front,back,formula,source,tags,extra
What is Principal Component Analysis (PCA)?,Dimensionality reduction technique that finds linear combinations of features with maximum variance,,ML Fundamentals,PCA dimensionality-reduction,Used to reduce feature space while preserving most information
What is the covariance matrix formula in PCA?,Matrix representing relationships between features,\[Cov(X) = \frac{1}{n}X^TX\],TOPICS.md,PCA covariance-matrix formula,Used to find principal components via eigendecomposition
How are principal components calculated?,Linear combinations of original features using eigenvectors,\[PC = X \cdot v\],TOPICS.md,PCA principal-components,"v are eigenvectors of the covariance matrix, ordered by eigenvalue magnitude"
What does explained variance tell us in PCA?,Proportion of dataset variance captured by each principal component,,ML Fundamentals,PCA explained-variance,Used to decide how many components to keep
What is K-means clustering?,Partitioning algorithm that divides data into k clusters by minimizing within-cluster variance,,ML Fundamentals,clustering k-means,Assumes spherical clusters of similar size
What is the K-means objective function?,Minimize within-cluster sum of squared distances,\[\min \sum_i \sum_{x \in C_i} ||x - \mu_i||^2\],TOPICS.md,k-means objective-function,Also called within-cluster sum of squares (WCSS) or inertia
How do you update centroids in K-means?,Take the mean of all points assigned to each cluster,\[\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i} x\],TOPICS.md,k-means centroid-update,This is the M-step in Lloyd's algorithm
What are the two steps of Lloyd's K-means algorithm?,Assignment step (assign points to nearest centroid) and Update step (recalculate centroids),,ML Fundamentals,k-means lloyds-algorithm,Alternates between these steps until convergence
What is the EM algorithm?,Expectation-Maximization algorithm for finding parameters when latent variables exist,,ML Fundamentals,EM algorithm latent-variables,Commonly used for Gaussian Mixture Models
What is the E-step in EM algorithm?,Calculate expectation of log-likelihood given current parameters,"\[Q(θ|θ^{(t)}) = E[\log L(θ|X,Z)|X,θ^{(t)}]\]",TOPICS.md,EM e-step expectation,Estimates probability of latent variable assignments
What is the M-step in EM algorithm?,Find parameters that maximize the expectation from E-step,\[θ^{(t+1)} = \arg\max Q(θ|θ^{(t)})\],TOPICS.md,EM m-step maximization,Updates model parameters based on expected latent assignments
What is a neural network?,"Computational model inspired by biological neurons, with layers of interconnected nodes",,ML Fundamentals,neural-networks definition,Universal function approximators capable of learning complex patterns
What is the forward pass in neural networks?,Process of computing output by propagating input through network layers,\[h = \sigma(Wx + b)\],TOPICS.md,neural-networks forward-pass,"σ is activation function (ReLU, sigmoid, tanh), W is weights, b is bias"
What is backpropagation?,Algorithm for computing gradients of loss function with respect to network weights,\[\frac{\partial L}{\partial W} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial W}\],TOPICS.md,neural-networks backpropagation,Uses chain rule to propagate error backwards through network
What is the ReLU activation function?,"Rectified Linear Unit: outputs input if positive, zero otherwise","\[ReLU(x) = \max(0, x)\]",ML Fundamentals,neural-networks activation-function relu,"Most popular activation function, helps with vanishing gradient problem"
What is gradient descent?,Optimization algorithm that iteratively updates parameters in direction of steepest descent,\[θ = θ - α∇J(θ)\],ML Fundamentals,optimization gradient-descent,"α is learning rate, ∇J(θ) is gradient of cost function"
What is linear regression?,Statistical method for modeling relationship between dependent variable and independent variables,\[y = β_0 + β_1x_1 + β_2x_2 + ... + ε\],ML Fundamentals,linear-regression supervised-learning,Assumes linear relationship between features and target
What is the normal equation for linear regression?,Closed-form solution for optimal parameters,\[β = (X^TX)^{-1}X^Ty\],ML Fundamentals,linear-regression normal-equation,Minimizes least squares error analytically
What is Ridge regression?,Linear regression with L2 regularization to prevent overfitting,\[\hat{β} = (X^TX + λI)^{-1}X^Ty\],TOPICS.md,ridge-regression regularization,λ is regularization parameter; higher λ means more regularization
What is the difference between Ridge and Lasso regression?,"Ridge uses L2 penalty (squared weights), Lasso uses L1 penalty (absolute weights)",,ML Fundamentals,regularization ridge lasso,"Lasso can shrink coefficients to zero (feature selection), Ridge cannot"
What is logistic regression?,Classification algorithm using logistic function to model probability of binary outcomes,\[p = \frac{1}{1 + e^{-z}}\],ML Fundamentals,logistic-regression classification,"z = β₀ + β₁x₁ + β₂x₂ + ..., outputs probability between 0 and 1"
What is the softmax function?,Generalization of logistic function for multi-class classification,\[p_i = \frac{\exp(w_i^T x)}{\sum_j \exp(w_j^T x)}\],TOPICS.md,logistic-regression softmax multi-class,Outputs probability distribution over all classes
What is entropy in decision trees?,Measure of impurity or randomness in a dataset,\[H(S) = -\sum p_i \log_2(p_i)\],TOPICS.md,decision-trees entropy,Lower entropy means more homogeneous (pure) dataset
What is information gain?,Reduction in entropy after splitting on an attribute,"\[IG(S,A) = H(S) - \sum \frac{|S_v|}{|S|} H(S_v)\]",TOPICS.md,decision-trees information-gain,Used to select best attribute for splitting at each node
What is Gini impurity?,Alternative to entropy for measuring node impurity,\[Gini = 1 - \sum p_i^2\],ML Fundamentals,decision-trees gini-impurity,"Computationally faster than entropy, similar results"
What is Random Forest?,Ensemble method combining multiple decision trees with bagging,,ML Fundamentals,ensemble random-forest decision-trees,Reduces variance and overfitting compared to single decision tree
What is Support Vector Machine (SVM)?,Classification algorithm that finds optimal hyperplane maximizing margin between classes,,ML Fundamentals,SVM classification margin,Support vectors are data points closest to decision boundary
What is the SVM primal objective function?,Minimize weights while allowing some misclassification,\[\min \frac{1}{2}||w||^2 + C\sum ξ_i\],TOPICS.md,SVM primal objective,C controls trade-off between margin size and misclassification penalty
What is the SVM constraint?,Points must be on correct side of margin or pay penalty,\[y_i(w^Tx_i + b) ≥ 1 - ξ_i\],TOPICS.md,SVM constraint slack-variables,ξᵢ are slack variables allowing soft margin
What is the kernel trick in SVM?,Technique to implicitly map data to higher-dimensional space for non-linear classification,,ML Fundamentals,SVM kernel-trick,"Common kernels: linear, polynomial, RBF (Gaussian)"
What is Naive Bayes classifier?,Probabilistic classifier based on Bayes theorem with strong independence assumption,\[P(C|X) = \frac{P(X|C)P(C)}{P(X)}\],ML Fundamentals,naive-bayes classification bayes-theorem,Assumes features are conditionally independent given class
What is the naive independence assumption?,Features are conditionally independent given the class label,"\[P(x_1,...,x_n|C) = \prod P(x_i|C)\]",ML Fundamentals,naive-bayes independence-assumption,Simplifies computation but often violated in practice
What is Laplace smoothing?,Technique to handle zero probabilities by adding small constant to counts,"\[P(x_i|C) = \frac{count(x_i,C) + α}{count(C) + α|V|}\]",ML Fundamentals,naive-bayes laplace-smoothing,"α is smoothing parameter (usually 1), |V| is vocabulary size"
What is the bias-variance tradeoff?,Fundamental tradeoff between model complexity and generalization ability,\[Error = Bias^2 + Variance + Noise\],ML Fundamentals,bias-variance tradeoff,"High bias = underfitting, high variance = overfitting"
What is bias in machine learning?,Error from overly simplistic assumptions in learning algorithm,,ML Fundamentals,bias underfitting,High bias leads to underfitting and poor performance on training data
What is variance in machine learning?,Error from sensitivity to small fluctuations in training set,,ML Fundamentals,variance overfitting,High variance leads to overfitting and poor generalization
What is Bayes error?,Lowest possible error rate for any classifier on a given problem,,ML Fundamentals,bayes-error irreducible-error,Represents irreducible error due to noise and overlapping classes
What is accuracy?,Fraction of correct predictions out of total predictions,\[Accuracy = \frac{TP + TN}{TP + TN + FP + FN}\],ML Fundamentals,evaluation accuracy,Can be misleading with imbalanced datasets
What is precision?,Fraction of true positives among predicted positives,\[Precision = \frac{TP}{TP + FP}\],TOPICS.md,evaluation precision,"Answers: Of all positive predictions, how many were correct?"
What is recall (sensitivity)?,Fraction of true positives among actual positives,\[Recall = \frac{TP}{TP + FN}\],TOPICS.md,evaluation recall sensitivity,"Answers: Of all actual positives, how many were found?"
What is F1-score?,Harmonic mean of precision and recall,\[F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}\],TOPICS.md,evaluation f1-score,"Balances precision and recall, useful for imbalanced datasets"
What is specificity?,Fraction of true negatives among actual negatives,\[Specificity = \frac{TN}{TN + FP}\],ML Fundamentals,evaluation specificity,"Answers: Of all actual negatives, how many were correctly identified?"
What are the 6 phases of CRISP-DM?,"Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment",,ML Fundamentals,CRISP-DM methodology,Iterative process for data mining projects
What is data normalization?,"Scaling features to have similar ranges, typically [0,1]",\[x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}\],ML Fundamentals,preprocessing normalization,Prevents features with large scales from dominating
What is data standardization?,Scaling features to have zero mean and unit variance,\[x_{std} = \frac{x - μ}{σ}\],ML Fundamentals,preprocessing standardization z-score,"Results in standard normal distribution (mean=0, std=1)"
What is cross-validation?,Technique for assessing model performance by splitting data into multiple train/validation sets,,ML Fundamentals,evaluation cross-validation,"K-fold CV divides data into k subsets, trains k times"
What is overfitting?,Model performs well on training data but poorly on unseen data,,ML Fundamentals,overfitting generalization,Model memorizes training data instead of learning patterns
What is underfitting?,Model is too simple to capture underlying patterns in data,,ML Fundamentals,underfitting bias,Results in poor performance on both training and test data
